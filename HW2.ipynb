{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30810 Intro to Text Analytics 2018\n",
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_trainset = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "raw_trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### Extract Tokens from Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(rawtext):\n",
    "    #Split the Raw Text into Tokens by using the Regular Expression Filter\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_words = tokenizer.tokenize(rawtext)\n",
    "\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "# Combine the stopwords\n",
    "STOP_WORDS = stopwords_nltk_en.union({\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"})\n",
    "\n",
    "def remove_stopwords(token_words):\n",
    "    rm_stop_words_tokens = ([word for word in token_words if word.lower() not in STOP_WORDS])\n",
    "\n",
    "    return rm_stop_words_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def decapitalize(rm_stop_words_tokens):\n",
    "    rmcap_token_words = [word.lower() for word in rm_stop_words_tokens]\n",
    "    \n",
    "    return rmcap_token_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Salutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_salutation(rmcap_token_words):\n",
    "    salutation = ['mr','mrs','mss','dr','phd','prof','rev', 'professor']\n",
    "    rmsalu_token_words = ([word for word in rmcap_token_words if word.lower() not in salutation])\n",
    "\n",
    "    return rmsalu_token_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(rmsalu_token_words):\n",
    "    rm_num_token_words = ([word for word in rmsalu_token_words if not word.isdigit()])\n",
    "\n",
    "    return rm_num_token_words\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('j' or 'J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('v' or 'V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('n' or 'N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('r' or 'R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return 'n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(rm_num_token_words, verbose=False):\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(rm_num_token_words):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        lemma_words.append(wnl.lemmatize(word, wtag)) # -> get lemma for word with tag\n",
    "    if (verbose):\n",
    "        print('Lemmas : ' + str(lemma_words[0:10]))\n",
    "        \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(rawtext):\n",
    "    return lemmatize(\n",
    "         remove_numbers(\n",
    "             remove_salutation(\n",
    "                 remove_stopwords(\n",
    "                     decapitalize(\n",
    "                         extract_tokens(\n",
    "                             rawtext))))))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle = raw_trainset.copy()\n",
    "df_handle['tokens'] = df_handle['content'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# statistically check how important a word is to an article category\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l1')\n",
    "document_token_strings = [' '.join(tokens) for tokens in df_handle.tokens]\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document_token_strings).toarray()\n",
    "df_handle['tfidf'] = list(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(df_handle.head().iloc[0].tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle = df_handle[['content', 'tokens', 'tfidf', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle.to_csv('./tfidf_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_handle, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: ' + repr(len(train)))\n",
    "print('Test: ' + repr(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(value1, value2):\n",
    "    return np.linalg.norm(value1-value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours(vector):\n",
    "    ret = []\n",
    "    for index, row in train.iterrows():\n",
    "        ret.append([row.category, euclideanDistance(row['tfidf'], vector)])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "tp = 0\n",
    "sample_size = 5\n",
    "\n",
    "for i in range(len(test)):\n",
    "    nearest_neighbours = get_nearest_neighbours(test.iloc[i]['tfidf'])\n",
    "    sort_NN = list(sorted(nearest_neighbours, key=lambda x: x[1], reverse=True)) # sort the returned list of vectors in order of highest to loweest distance\n",
    "    \n",
    "    k=10\n",
    "    votes = defaultdict(int) # create dictionary of votes and tallied votes\n",
    "    for j in range(k):\n",
    "        votes[sort_NN[j][0]] += 1\n",
    "    final_vote = list(sorted(votes.items(), key=itemgetter(1), reverse=True ))[0][0] # put highest voted value first\n",
    "    tp += int(final_vote == test.iloc[j]['category'])\n",
    "    \n",
    "accuracy = tp / len(test)\n",
    "    # logic for choosing what got voted for\n",
    "    # if category voted for equals best label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(final_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Cross-Validation Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(raw_trainset)\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "train_input_vectors = tfidf_transformer.fit_transform(train.content)\n",
    "train_labels = train.category\n",
    "scores = cross_val_score(MultinomialNB(), train_input_vectors, train_labels, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"Article %s:\" % i)\n",
    "    print(data.content[i].split('.')[0])\n",
    "    print(\"Model prediction: %s\" % model.predict(X_train_tfidf[i])[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Sklearn Models\n",
    "\n",
    "Try out more models from Sklearn and report their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = [\n",
    "    \"Multinomial Naive Bayes\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"AdaBoost\",\n",
    "    \"Linear SVM\", \n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",  \n",
    "#     \"Naive Bayes\",\n",
    "#     \"Neural Net\",\n",
    "#     \"Gaussian Process\",\n",
    "         ]\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(alpha=1.0),\n",
    "    KNeighborsClassifier(3),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1), # took to long to run\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)), # needs a 'dense matrix'?\n",
    "#     GaussianNB(), # also needs a 'dense matrix'?\n",
    "]\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(classifier, train_input_vectors, train_labels, cv=k_folds)\n",
    "    average_accuracy = np.mean(scores)\n",
    "\n",
    "    print(\"%s average accuracy (%d-fold x-val): %f\" \n",
    "          % (name, k_folds, average_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "```\n",
    "Multinomial Naive Bayes average accuracy (10-fold x-val): 0.950307\n",
    "Nearest Neighbors average accuracy (10-fold x-val): 0.892863\n",
    "AdaBoost average accuracy (10-fold x-val): 0.712065\n",
    "Linear SVM average accuracy (10-fold x-val): 0.224502\n",
    "RBF SVM average accuracy (10-fold x-val): 0.944214\n",
    "Decision Tree average accuracy (10-fold x-val): 0.653080\n",
    "Random Forest average accuracy (10-fold x-val): 0.323166\n",
    "```\n",
    "\n",
    "The Naive Bayes and RBF SVM are by far the most-promising. We should look into tweaking these models further to see if we can improve on the results.\n",
    "\n",
    "The Nearest Neighbour model also performs well. We check to see which value of k provides the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 0\n",
    "best_k_accuracy = 0\n",
    "\n",
    "for k in range(1,100):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(KNeighborsClassifier(k), train_input_vectors, train_labels, cv=k_folds)\n",
    "    average_accuracy = np.mean(scores)\n",
    "    \n",
    "    if average_accuracy > best_k_accuracy:\n",
    "        best_k_accuracy = average_accuracy\n",
    "        best_k = k\n",
    "\n",
    "    print(\"KNN (k=%d) average accuracy (%d-fold x-val): %f\" \n",
    "          % (k, k_folds, average_accuracy))\n",
    "    \n",
    "\n",
    "print(\"Best k value is %d\" % best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing KNN uses k=6, accuracy = 0.916898. Still not as good as the NB or RBF SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting ensemble of the best three classifiers from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "best_classifiers = [\n",
    "    ('MultinomialNB', MultinomialNB(alpha=1.0)),\n",
    "    ('KNN', KNeighborsClassifier(6)),\n",
    "    ('SVM', SVC(gamma=2, C=1)),\n",
    "]\n",
    "\n",
    "voting_ensemble = sklearn.ensemble.VotingClassifier(best_classifiers)\n",
    "scores = cross_val_score(voting_ensemble, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.9486095170000001\n",
    "\n",
    "This is slightly worse than MultinomialNB from our previous experiment. We have run these experiments several times and sometimes MultinomialNB is worse (~93% accuracy), but in general it seems that MultimonialNB is about as good as or better than the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging ensemble of the best classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = MultinomialNB(alpha=1.0)\n",
    "bagging_ensemble = sklearn.ensemble.BaggingClassifier(base_classifier)\n",
    "scores = cross_val_score(bagging_ensemble, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.926422\n",
    "\n",
    "Again this is slightly worse than the standard MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = MultinomialNB(alpha=1.0)\n",
    "boosting_classifier = sklearn.ensemble.AdaBoostClassifier(base_classifier)\n",
    "scores = cross_val_score(boosting_classifier, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.666899\n",
    "\n",
    "This is significantly worse than standard MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "A voting ensemble, bagging ensemble, and boosting ensemble all achieved worse results than a standard MultinomialNB model. For this reason we will not use an ensemble for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
