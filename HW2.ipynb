{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30810 Intro to Text Analytics 2018\n",
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud as wc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_handle = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### Extract Tokens from Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(rawtext):\n",
    "    \"\"\"Split raw text into tokens.\"\"\"\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    return tokenizer.tokenize(rawtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "# Combine nltk stopwords with some extra ones\n",
    "STOP_WORDS = stopwords_nltk_en.union({\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"})\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def decapitalize(tokens):\n",
    "    return [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Salutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALUTATIONS = ('mr','mrs','mss','dr','phd','prof','rev','professor')\n",
    "\n",
    "def remove_salutations(tokens):\n",
    "    return [word for word in tokens if word.lower() not in SALUTATIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(tokens):\n",
    "    return [word for word in tokens if not word.isdigit()]         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('j' or 'J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('v' or 'V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('n' or 'N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('r' or 'R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return 'n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(tokens):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        lemma_words.append(wnl.lemmatize(word, wtag) if len(word)>2 else word) # -> get lemma for word with tag\n",
    "\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(rawtext):\n",
    "    return lemmatize(\n",
    "         remove_numbers(\n",
    "             remove_salutations(\n",
    "                 remove_stopwords(\n",
    "                     decapitalize(\n",
    "                         extract_tokens(\n",
    "                             rawtext))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tokenizer\n",
    "We'll test the tokenizer on a short sample of text to check for any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = df_handle.content[0][:201]\n",
    "print(sample_text)\n",
    "print(tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look fine except for the second token 'bos', which should be 'boss'. This is because the lemmatizer thinks that 'boss' is a plural, and so converts it to the singular form 'bos':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize(['boss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bug in the lemmatizer, so we cannot fix it. Despite this minor issue, we will continue to use lemmatization as it is very useful even if it sometimes makes mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['content'].apply(tokenize)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# statistically check how important a word is to an article category\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l1')\n",
    "document_token_strings = [' '.join(tokens) for tokens in df_handle.tokens]\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document_token_strings).toarray()\n",
    "df_handle['tfidf'] = list(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(df_handle.head().iloc[0].tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle = df_handle[['content', 'tokens', 'tfidf', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle.to_csv('./tfidf_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_handle, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: ' + repr(len(train)))\n",
    "print('Test: ' + repr(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(value1, value2):\n",
    "    return np.linalg.norm(value1-value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours(vector):\n",
    "    ret = []\n",
    "    for index, row in train.iterrows():\n",
    "        ret.append([row.category, euclideanDistance(row['tfidf'], vector)])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_Tokens = []\n",
    "for index in range(len(train)):\n",
    "    encoded_Tokens.append(le.fit_transform(df_handle.iloc[index]['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_Labels = le.fit_transform(df_handle['category'].tolist())\n",
    "encoded_Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(le.fit_transform(df_handle['tokens']), encoded_Labels, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Train the model using the training sets\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "# y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(len(encoded_Tokens)):\n",
    "#     modelknn = KNeighborsClassifier(n_neighbors=5)\n",
    "#     modelknn.fit(index, encoded_Labels)\n",
    "\n",
    "# pred_KNN = modelknn.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "tp = 0\n",
    "sample_size = 5\n",
    "\n",
    "for i in range(len(test)):\n",
    "    nearest_neighbours = get_nearest_neighbours(test.iloc[i]['tfidf'])\n",
    "    sort_NN = list(sorted(nearest_neighbours, key=lambda x: x[1], reverse=True)) # sort the returned list of vectors in order of highest to loweest distance\n",
    "    \n",
    "    k=10\n",
    "    votes = defaultdict(int) # create dictionary of votes and tallied votes\n",
    "    for j in range(k):\n",
    "        votes[sort_NN[j][0]] += 1\n",
    "    final_vote = list(sorted(votes.items(), key=itemgetter(1), reverse=True ))[0][0] # put highest voted value first\n",
    "    tp += int(final_vote == test.iloc[j]['category'])\n",
    "    \n",
    "accuracy = tp / len(test)\n",
    "    # logic for choosing what got voted for\n",
    "    # if category voted for equals best label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(final_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Cross-Validation Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_handle)\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "train_input_vectors = tfidf_transformer.fit_transform(train.content)\n",
    "train_labels = train.category\n",
    "scores = cross_val_score(MultinomialNB(), train_input_vectors, train_labels, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(train_input_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"Article %s:\" % i)\n",
    "    print(df_handle.content[i].split('.')[0])\n",
    "    print(\"Model prediction: %s\" % model.predict(train_input_vectors[i])[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## More Sklearn Models\n",
    "\n",
    "Try out more models from Sklearn and report their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = [\n",
    "    \"Multinomial Naive Bayes\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"AdaBoost\",\n",
    "    \"Linear SVM\", \n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",  \n",
    "#     \"Naive Bayes\",\n",
    "#     \"Neural Net\",\n",
    "#     \"Gaussian Process\",\n",
    "         ]\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(alpha=1.0),\n",
    "    KNeighborsClassifier(3),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1), # took to long to run\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)), # needs a 'dense matrix'?\n",
    "#     GaussianNB(), # also needs a 'dense matrix'?\n",
    "]\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(classifier, train_input_vectors, train_labels, cv=k_folds)\n",
    "    average_accuracy = np.mean(scores)\n",
    "\n",
    "    print(\"%s average accuracy (%d-fold x-val): %f\" \n",
    "          % (name, k_folds, average_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "```\n",
    "Multinomial Naive Bayes average accuracy (10-fold x-val): 0.950307\n",
    "Nearest Neighbors average accuracy (10-fold x-val): 0.892863\n",
    "AdaBoost average accuracy (10-fold x-val): 0.712065\n",
    "Linear SVM average accuracy (10-fold x-val): 0.224502\n",
    "RBF SVM average accuracy (10-fold x-val): 0.944214\n",
    "Decision Tree average accuracy (10-fold x-val): 0.653080\n",
    "Random Forest average accuracy (10-fold x-val): 0.323166\n",
    "```\n",
    "\n",
    "The Naive Bayes and RBF SVM are by far the most-promising. We should look into tweaking these models further to see if we can improve on the results.\n",
    "\n",
    "The Nearest Neighbour model also performs well. We check to see which value of k provides the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 0\n",
    "best_k_accuracy = 0\n",
    "\n",
    "for k in range(1,100):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(KNeighborsClassifier(k), train_input_vectors, train_labels, cv=k_folds)\n",
    "    average_accuracy = np.mean(scores)\n",
    "    \n",
    "    if average_accuracy > best_k_accuracy:\n",
    "        best_k_accuracy = average_accuracy\n",
    "        best_k = k\n",
    "\n",
    "    print(\"KNN (k=%d) average accuracy (%d-fold x-val): %f\" \n",
    "          % (k, k_folds, average_accuracy))\n",
    "    \n",
    "\n",
    "print(\"Best k value is %d\" % best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing KNN uses k=6, accuracy = 0.916898. Still not as good as the NB or RBF SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting ensemble of the best three classifiers from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "best_classifiers = [\n",
    "    ('MultinomialNB', MultinomialNB(alpha=1.0)),\n",
    "    ('KNN', KNeighborsClassifier(6)),\n",
    "    ('SVM', SVC(gamma=2, C=1)),\n",
    "]\n",
    "\n",
    "voting_ensemble = sklearn.ensemble.VotingClassifier(best_classifiers)\n",
    "scores = cross_val_score(voting_ensemble, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.9486095170000001\n",
    "\n",
    "This is slightly worse than MultinomialNB from our previous experiment. We have run these experiments several times and sometimes MultinomialNB is worse (~93% accuracy), but in general it seems that MultimonialNB is about as good as or better than the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging ensemble of the best classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = MultinomialNB(alpha=1.0)\n",
    "bagging_ensemble = sklearn.ensemble.BaggingClassifier(base_classifier)\n",
    "scores = cross_val_score(bagging_ensemble, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.926422\n",
    "\n",
    "Again this is slightly worse than the standard MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting ensemble of the best classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = MultinomialNB(alpha=1.0)\n",
    "boosting_classifier = sklearn.ensemble.AdaBoostClassifier(base_classifier)\n",
    "scores = cross_val_score(boosting_classifier, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.666899\n",
    "\n",
    "This is significantly worse than standard MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "A voting ensemble, bagging ensemble, and boosting ensemble all achieved worse results than a standard MultinomialNB model. For this reason we will not use an ensemble for our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Now we turn our attention to exploring and analysing the data that we have cleaned, transformed, tokenized and saved into our data frame. We will look at a number of different features of the data, including:\n",
    "- Class-Label Analysis\n",
    "    - Class Frequency and Balance\n",
    "    - Class Statistics\n",
    "    - Class Similarity\n",
    "- Word Clouds for Tokens and Nouns\n",
    "- Most-Common and Words, Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most-Common N-Grams\n",
    "\n",
    "We explore our corpus by looking at which words and phrases are most common. We break this down by document category in order to get a feel for which words are most indicative of a particular class, and therefore the words that should be considered most important by the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dist_to_df(freq_dist, top_n):\n",
    "    \"\"\"\n",
    "    Convert a frequency distribution to a Data Frame, with columns 'term' and 'count'.\n",
    "    \"\"\"\n",
    "    most_common_terms = []\n",
    "    \n",
    "    for term, count in freq_dist.most_common(top_n):\n",
    "        most_common_terms.append({'term': term, 'count': count})\n",
    "    \n",
    "    most_common_df = pd.DataFrame(most_common_terms)\n",
    "    most_common_df.sort_values(by='count', ascending=True, inplace=True)\n",
    "    \n",
    "    # Change term labels from being (x, y, z) to \"x y z\".\n",
    "    join_tuple = lambda terms: \" \".join(terms)\n",
    "    most_common_df.term = most_common_df.term.apply(join_tuple)\n",
    "\n",
    "    return most_common_df\n",
    "\n",
    "\n",
    "def plot_frequency_df(df, title, xlabel=\"Frequency\", ylabel=\"\", fileName=None):\n",
    "    \"\"\"\n",
    "    Plot a horizontal bar chart of the most common words in a given \n",
    "    dataframe, with columns 'term' and 'count'. The bars are sorted \n",
    "    so that the most-common word appears at the top.\n",
    "    \"\"\"\n",
    "    fig, axis = plt.subplots()\n",
    "\n",
    "    df.plot.barh(x='term', y='count', ax=axis)\n",
    "    \n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel(xlabel)\n",
    "    axis.set_ylabel(ylabel)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure to an image file.\n",
    "    if fileName is not None:\n",
    "        plt.savefig(\"images/\" + fileName)\n",
    "\n",
    "    # This should only be executed after plt.savefig(),\n",
    "    # otherwise the image saved will be blank.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ngrams(ngrams_data, n=1, file_name=None):\n",
    "    \"\"\"\n",
    "    Plot the frequency of n-grams in a Series, for a given n.\n",
    "    \"\"\"\n",
    "    # Get all the tokens for the corpus in a list.\n",
    "    all_tokens_list = [token for tokens in ngrams_data for token in tokens]\n",
    "\n",
    "    ngrams = nltk.ngrams(all_tokens_list, n)\n",
    "\n",
    "    # Get the frequency of each term in the corpus and create a DataFrame from it.\n",
    "    ngram_frequency = nltk.FreqDist(ngrams)\n",
    "    ngrams_df = freq_dist_to_df(ngram_frequency, 5)\n",
    "    \n",
    "    ngram_name = \"Word\"  # default\n",
    "    if n == 2:\n",
    "        ngram_name = \"Bigram\"\n",
    "    elif n == 3:\n",
    "        ngram_name = \"Trigram\"\n",
    "\n",
    "    plot_frequency_df(ngrams_df, title=\"Top-10 Most-Common %s for Entire Corpus\" % ngram_name,\n",
    "                      xlabel=\"Frequency\", ylabel=ngram_name, fileName=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=1, file_name=\"most-common-unigram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=2, file_name=\"most-common-bigram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=3, file_name=\"most-common-trigram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Most Common Unigrams](images/most-common-unigram.png)\n",
    "\n",
    "The most-common words for the entire corpus don't really give us a huge amount of insight about the data. The most-common word is \"year\" but this could be used in any of the contexts of the document categories. Similarly for the word \"make\", \"people\" and \"time\". However, one word that does stand out is \"game\", which we would primarily associate with sport. However, it is possible that this word is applicable to other categories too. We will investigate the most-common words per category later to see if the game-sport assocation is supported.\n",
    "\n",
    "![Most Common Bigrams](images/most-common-bigram.png)\n",
    "![Most Common Trigrams](images/most-common-trigram.png)\n",
    "\n",
    "We get more information from analysing the bigrams and trigrams than the single words (also known as *unigrams*). For example, we see that the top bigram is \"tell bbc\" and the three most common trigrams are \"tell bbc news\", \"bbc news website\" and \"bbc news radio\". This would seem to suggest that the source of our news articles is BBC News. In fact, this is also supported by the reference to former-British Prime Minister, Tony Blair, and former-British Conservative Party Leader, Michael Howard, which are referenced by the fifth most common bigram and fourth most common trigram, respectively. \n",
    "\n",
    "Furthermore, we see that there are two bigrams that stand out for their association with particular document categories. \"Prime Minister\" is definitely associated with politics and \"Chief Executive\" is similarly associated with the business topic. We will investigate the importance of these n-grams later when dicussing the important terms and phrases for classifying each topic. \n",
    "\n",
    "The fifth most common trigram is \"million dollar baby\". This could be a reference to the boxing film of the same name that won multiple Oscars in 2005. This trigram could definitely be associated with the entertainment category. However, seeing as it's a film that is related to sports, it could also be associated in some way to the sports category. We will investigate the overlap between categories and their similarities later on in our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-Label Analysis\n",
    "\n",
    "Let's look at the labels that we have been provided with for each of the documents in our corpus. There are five possible classes that the label can be: sport, entertainment, politics, technology and business. We also refer to our class labels as the *target label* of each document.\n",
    "\n",
    "### Class Frequency and Balance\n",
    "To start with, we look at how many documents of each class appear in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df_handle.groupby(df_handle.category).category.count()\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise these absolute numbers to get a breakdown of the percentage frequency of each target class. This makes it easy to compare the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution_normalised = (class_distribution / sum(class_distribution)) * 100\n",
    "# Plot a line representing the mean frequency of all classes.\n",
    "# This is the frequency each class would have if uniformly distributed.\n",
    "mean_freq = 100 / len(class_distribution)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.axhline(y=mean_freq, color='orange')\n",
    "class_distribution_normalised.plot.bar(ax=axis, rot=0)\n",
    "axis.set_xlabel(\"Target Class (Document Category)\")\n",
    "axis.set_ylabel(\"Frequency (% of Corpus)\")\n",
    "axis.set_title(\"Target Class Frequencies Across Corpus\")\n",
    "\n",
    "plt.savefig('images/target-class-frequencies.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Target Class Frequencies Image](images/target-class-frequencies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a slighly unbalanced target class distribution. This means that not all target labels appear as often as each other. For example, the sport category is most common, whereas the entertainment category is the least common. When building our model, we need to take this into account, to make sure that we are not overly biasing our model based on the distribution of target classes in the training data. In extreme cases, our model could underfit the data and end up predicting the most common label (sport) each time. \n",
    "\n",
    "While this would be easy to spot and fix, it is possible that the class distribtion will affect the model in more subtle ways. We will investigate and evaluate this later, when we have made our predictions for the test dataset.\n",
    "\n",
    "### Class Statistics\n",
    "\n",
    "Now, we look at some meta-statistics about each document. We compare the length of each document as well as the number of unique words in each document. We breakdown this analysis per-target class.\n",
    "\n",
    "#### Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['content_length'] = df_handle.content.apply(len)\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "df_handle.plot.hist(by='content_length', bins=5, ax=axis[0], figsize=(10,4), legend=None)\n",
    "axis[0].set_xlabel(\"Document Length\")\n",
    "axis[0].set_title(\"Document Length Distribution\")\n",
    "\n",
    "clipped_content_length_data = df_handle[(df_handle.content_length > 1000) & (df_handle.content_length < 5000)]\n",
    "clipped_content_length_data.plot.hist(by='content_length', bins=5, ax=axis[1], figsize=(10, 4), legend=None)\n",
    "axis[1].set_xlabel(\"Document Length\")\n",
    "axis[1].set_title(\"Document Length Distribution (clipped)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/document-length-distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Document Length Distributions](images/document-length-distribution.png)\n",
    "\n",
    "From the first histogram on the left, we see that the overwhelming majority of documents have a length between 1000 and 5000 characters. We don't get a huge amount of information from this plot so we replot the data, except that we filter the dataset so that we only include documents whose length is within this interval. This process is known as *clipping*. The clipped data is plotted on the right histogram.\n",
    "\n",
    "From the second histogram, we get more useful data. We see most of the documents have a length between 1000 and 2500 characters. It is important that we are aware that the distribution of the lengths of each document is not uniform. We also break this down per-target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_length_per_class = df_handle.groupby(df_handle.category).content_length.mean()\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "\n",
    "content_length_per_class.plot.bar(ax=axis, rot=0)\n",
    "\n",
    "# We plot the average document length for reference.\n",
    "mean_doc_length = df_handle['content_length'].mean()\n",
    "axis.axhline(y=mean_doc_length, color='orange')\n",
    "\n",
    "axis.set_xlabel(\"Target Class (Document Category)\")\n",
    "axis.set_ylabel(\"Mean Document Length\")\n",
    "axis.set_title(\"Mean Document Length per Target Class\")\n",
    "\n",
    "plt.savefig(\"images/document-length-per-class.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mean Document Length per Target Class Bar Chart](images/document-length-per-class.png)\n",
    "\n",
    "Again, we see that we don't have uniform distribution of document length. This is important to recall, since, when we are using methods to vectorise the documents, if we don't take into account the documents' length, then we will be unfairly biasing longer or shorter documents (depending on the vectorising method). \n",
    "\n",
    "For example, if we are calculating the frequency of a term, comparing this across documents that have different lengths is unfair. A word that appears 10 times in a 500 character document is far more prominent that a word that appears 20 times in a 5000 chracter document. So we need to make sure that we normalise our frequency calculation (e.g. during TFIDF vectorisation) by the length of the doucment the term appears in. We will return to this point later when we are doing the document vectorisation.\n",
    "\n",
    "### Class Similarity\n",
    "\n",
    "TODO (mm): Calculate the overlap/cosine similarity between each of the classes. Mention how if we have classes that are often similar, our classifier model could become confused by them. E.g An incorrect 51-49-0-0-0 is better than a correct 21-20-20-20-19 prediction if the document is solely about a single topic with no overlap with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndPlotWordCloud(tokens):\n",
    "    # The word cloud uses a sentence string not a list,\n",
    "    # so we convert our tokens to a single string.\n",
    "    tokens_string = \" \".join(tokens)\n",
    "    \n",
    "    # Create the wordcloud object.\n",
    "    wordcloud = wc.WordCloud(width=1600, height=800).generate(tokens_string)\n",
    "    \n",
    "    # Matplotlib settings.\n",
    "    plt.figure(figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "\n",
    "def getAllTokens(df):\n",
    "    \"\"\"\n",
    "    Convert the tokens column in a dataframe into a list of strings of every \n",
    "    token in the tokens column.\n",
    "    \"\"\"\n",
    "    return [token for tokens in df['tokens'] for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the wordcloud image and show it. We also save it to a file called *corpus_wordcloud.png*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all the tokens from our data.\n",
    "all_tokens_list = getAllTokens(df_handle)\n",
    "\n",
    "wordcloud = createAndPlotWordCloud(all_tokens_list)\n",
    "wordcloud.to_file(\"images/corpus_wordcloud.png\")\n",
    "\n",
    "# Delete this since it's a variable that will be used later.\n",
    "del all_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(mm): Analyse.\n",
    "\n",
    "![Wordcloud for Tokens of the Entire Corpus](images/corpus_wordcloud.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
