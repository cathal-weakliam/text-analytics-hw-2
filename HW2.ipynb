{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30810 Intro to Text Analytics 2018\n",
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud as wc\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content       category\n",
       "0  French boss to leave EADS The French co-head o...       business\n",
       "1  Gamers could drive high-definition TV, films, ...           tech\n",
       "2  Stalemate in pension strike talks Talks aimed ...       politics\n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...  entertainment\n",
       "4  Tautou 'to star in Da Vinci film' French actre...  entertainment"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle = pd.read_csv('trainingset.csv',sep='^',header=0)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### Extract Tokens from Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(rawtext):\n",
    "    \"\"\"Split raw text into tokens.\"\"\"\n",
    "    pattern = r'\\w+'\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    return tokenizer.tokenize(rawtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "# Combine nltk stopwords with some extra ones\n",
    "STOP_WORDS = stopwords_nltk_en.union({\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"})\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def decapitalize(tokens):\n",
    "    return [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Salutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALUTATIONS = ('mr','mrs','mss','dr','phd','prof','rev','professor')\n",
    "\n",
    "def remove_salutations(tokens):\n",
    "    return [word for word in tokens if word.lower() not in SALUTATIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(tokens):\n",
    "    return [word for word in tokens if not word.isdigit()]         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_tag(treebank_tag):\n",
    "    if treebank_tag.startswith('j' or 'J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('v' or 'V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('n' or 'N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('r' or 'R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return 'n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    for word, tag in nltk.pos_tag(tokens):\n",
    "        firstletter = tag[0].lower() # -> get the first letter of tag and put them decapitalized form\n",
    "        wtag = transfer_tag(firstletter) # -> extract the word's tag (noun, verb, adverb, adjective)\n",
    "        lemma_words.append(wnl.lemmatize(word, wtag) if len(word)>2 else word) # -> get lemma for word with tag\n",
    "\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(rawtext):\n",
    "    return lemmatize(\n",
    "         remove_numbers(\n",
    "             remove_salutations(\n",
    "                 remove_stopwords(\n",
    "                     decapitalize(\n",
    "                         extract_tokens(\n",
    "                             rawtext))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tokenizer\n",
    "We'll test the tokenizer on a short sample of text to check for any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French boss to leave EADS The French co-head of European defence and aerospace group EADS Philippe Camus is to leave his post. Mr Camus said in a statement that he has accepted the invitation to return\n",
      "['french', 'bos', 'leave', 'eads', 'french', 'head', 'european', 'defence', 'aerospace', 'group', 'eads', 'philippe', 'camus', 'leave', 'post', 'camus', 'statement', 'accept', 'invitation', 'return']\n"
     ]
    }
   ],
   "source": [
    "sample_text = df_handle.content[0][:201]\n",
    "print(sample_text)\n",
    "print(tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look fine except for the second token 'bos', which should be 'boss'. This is because the lemmatizer thinks that 'boss' is a plural, and so converts it to the singular form 'bos':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(['boss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bug in the lemmatizer, so we cannot fix it. Despite this minor issue, we will continue to use lemmatization as it is very useful even if it sometimes makes mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>[french, bos, leave, eads, french, head, europ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>[gamers, drive, high, definition, tv, film, ga...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>[stalemate, pension, strike, talk, talk, aim, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>[johnny, denise, lose, passport, johnny, vaugh...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>[tautou, star, da, vinci, film, french, actres...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  French boss to leave EADS The French co-head o...   \n",
       "1  Gamers could drive high-definition TV, films, ...   \n",
       "2  Stalemate in pension strike talks Talks aimed ...   \n",
       "3  Johnny and Denise lose Passport Johnny Vaughan...   \n",
       "4  Tautou 'to star in Da Vinci film' French actre...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [french, bos, leave, eads, french, head, europ...   \n",
       "1  [gamers, drive, high, definition, tv, film, ga...   \n",
       "2  [stalemate, pension, strike, talk, talk, aim, ...   \n",
       "3  [johnny, denise, lose, passport, johnny, vaugh...   \n",
       "4  [tautou, star, da, vinci, film, french, actres...   \n",
       "\n",
       "                                               tfidf       category  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle['tokens'] = df_handle['content'].apply(tokenize)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# statistically check how important a word is to an article category\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l1')\n",
    "document_token_strings = [' '.join(tokens) for tokens in df_handle.tokens]\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document_token_strings).toarray()\n",
    "df_handle['tfidf'] = list(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_handle.head().iloc[0].tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle = df_handle[['content', 'tokens', 'tfidf', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French boss to leave EADS The French co-head o...</td>\n",
       "      <td>[french, bos, leave, eads, french, head, europ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gamers could drive high-definition TV, films, ...</td>\n",
       "      <td>[gamers, drive, high, definition, tv, film, ga...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stalemate in pension strike talks Talks aimed ...</td>\n",
       "      <td>[stalemate, pension, strike, talk, talk, aim, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johnny and Denise lose Passport Johnny Vaughan...</td>\n",
       "      <td>[johnny, denise, lose, passport, johnny, vaugh...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tautou 'to star in Da Vinci film' French actre...</td>\n",
       "      <td>[tautou, star, da, vinci, film, french, actres...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Media seek Jackson 'juror' notes Reporters cov...</td>\n",
       "      <td>[medium, seek, jackson, juror, note, reporter,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Horror film heads US box office A low-budget h...</td>\n",
       "      <td>[horror, film, head, box, office, low, budget,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kerr frustrated at victory margin Republic of ...</td>\n",
       "      <td>[kerr, frustrate, victory, margin, republic, i...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US casino 'tricks' face ban in UK Controversia...</td>\n",
       "      <td>[casino, trick, face, ban, uk, controversial, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Klinsmann issues Lehmann warning Germany coach...</td>\n",
       "      <td>[klinsmann, issue, lehmann, warn, germany, coa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSE 'sets date for takeover deal' The London S...</td>\n",
       "      <td>[lse, set, date, takeover, deal, london, stock...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Councils 'must find Gypsy sites' Ministers are...</td>\n",
       "      <td>[council, find, gypsy, site, minister, tell, c...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Low-budget film wins Cesar A film that follows...</td>\n",
       "      <td>[low, budget, film, win, cesar, film, group, a...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Women in film 'are earning less' Women in the ...</td>\n",
       "      <td>[woman, film, earn, woman, uk, film, industry,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Falconer rebuts 'charade' claims Concessions o...</td>\n",
       "      <td>[falconer, rebuts, charade, claim, concession,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Toxic web links help virus spread Virus writer...</td>\n",
       "      <td>[toxic, web, link, virus, spread, virus, write...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FA probes crowd trouble The FA is to take acti...</td>\n",
       "      <td>[fa, probe, crowd, trouble, fa, action, troubl...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>India's rupee hits five-year high India's rupe...</td>\n",
       "      <td>[india, rupee, hit, year, high, india, rupee, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>California sets fines for spyware The makers o...</td>\n",
       "      <td>[california, set, fine, spyware, maker, comput...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Defiant hunts put ban to the test Thousands of...</td>\n",
       "      <td>[defiant, hunt, put, ban, test, thousand, hunt...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bristol City 2-1 Milton Keynes Leroy Lita took...</td>\n",
       "      <td>[bristol, city, milton, keynes, leroy, lita, g...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Smart search lets art fans browse If you don't...</td>\n",
       "      <td>[smart, search, let, art, fan, browse, art, se...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Michael film signals 'retirement' Singer Georg...</td>\n",
       "      <td>[michael, film, signal, retirement, singer, ge...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kilroy-Silk attacked with slurry Euro MP Rober...</td>\n",
       "      <td>[kilroy, silk, attack, slurry, euro, mp, rober...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sundance to honour foreign films International...</td>\n",
       "      <td>[sundance, honour, foreign, film, internationa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wolves appoint Hoddle as manager Glenn Hoddle ...</td>\n",
       "      <td>[wolf, appoint, hoddle, manager, glenn, hoddle...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Benitez deflects blame from Dudek Liverpool ma...</td>\n",
       "      <td>[benitez, deflects, blame, dudek, liverpool, m...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Giant waves damage S Asia economy Governments,...</td>\n",
       "      <td>[giant, wave, damage, asia, economy, governmen...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Dutch watch Van Gogh's last film The last film...</td>\n",
       "      <td>[dutch, watch, van, gogh, film, film, make, sl...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GB quartet get cross country call Four British...</td>\n",
       "      <td>[gb, quartet, cross, country, call, british, a...</td>\n",
       "      <td>[0.011810634137970098, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>Labour attacked on Howard poster Labour has be...</td>\n",
       "      <td>[labour, attack, howard, poster, labour, accus...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>BBC to pour Â£9m into new comedy The BBC is to...</td>\n",
       "      <td>[bbc, pour, â, 9m, comedy, bbc, invest, â, 9m,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>Be careful how you code A new European directi...</td>\n",
       "      <td>[careful, code, european, directive, put, soft...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>US actor Ossie Davis found dead US actor Ossie...</td>\n",
       "      <td>[actor, ossie, davis, find, dead, actor, ossie...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>Doors open at biggest gadget fair Thousands of...</td>\n",
       "      <td>[door, open, big, gadget, fair, thousand, tech...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>German business confidence slides German busin...</td>\n",
       "      <td>[german, business, confidence, slides, german,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>Wenger rules out new keeper Arsenal boss Arsen...</td>\n",
       "      <td>[wenger, rule, keeper, arsenal, bos, arsene, w...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>The year search became personal The odds are t...</td>\n",
       "      <td>[year, search, personal, odds, fire, browser, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>Robinson answers critics England captain Jason...</td>\n",
       "      <td>[robinson, answer, critic, england, captain, j...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>Federer breezes into semi-finals Roger Federer...</td>\n",
       "      <td>[federer, breeze, semi, final, roger, federer,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Blair dismisses quit claim report Tony Blair h...</td>\n",
       "      <td>[blair, dismiss, quit, claim, report, tony, bl...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>India's Reliance family feud heats up The ongo...</td>\n",
       "      <td>[india, reliance, family, feud, heat, ongoing,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>Mixed reaction to Man Utd offer Shares in Manc...</td>\n",
       "      <td>[mixed, reaction, man, utd, offer, share, manc...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>Berlin hails European cinema Organisers say th...</td>\n",
       "      <td>[berlin, hail, european, cinema, organiser, ye...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>Roundabout continues nostalgia trip The new bi...</td>\n",
       "      <td>[roundabout, continue, nostalgia, trip, big, s...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>Holmes starts 2005 with GB events Kelly Holmes...</td>\n",
       "      <td>[holmes, start, gb, event, kelly, holmes, star...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>Downing injury mars Uefa victory Middlesbrough...</td>\n",
       "      <td>[down, injury, mar, uefa, victory, middlesbrou...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>More reforms ahead says Milburn Labour will co...</td>\n",
       "      <td>[reform, ahead, milburn, labour, continue, pur...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>Two Nigerian banks set to merge Nigerian banks...</td>\n",
       "      <td>[nigerian, bank, set, merge, nigerian, bank, u...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>Career honour for actor DiCaprio Actor Leonard...</td>\n",
       "      <td>[career, honour, actor, dicaprio, actor, leona...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>Virus poses as Christmas e-mail Security firms...</td>\n",
       "      <td>[virus, pose, christmas, mail, security, firm,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>Abbas 'will not tolerate' attacks Palestinian ...</td>\n",
       "      <td>[abbas, tolerate, attack, palestinian, leader,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>Wi-fi web reaches farmers in Peru A network of...</td>\n",
       "      <td>[wi, fi, web, reach, farmer, peru, network, co...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>Business fears over sluggish EU economy As Eur...</td>\n",
       "      <td>[business, fear, sluggish, eu, economy, europe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>Tory candidate quits over remark A Conservativ...</td>\n",
       "      <td>[tory, candidate, quits, remark, conservative,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>Millions to miss out on the net By 2025, 40% o...</td>\n",
       "      <td>[million, miss, net, uk, population, internet,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>Peace demo appeal rejected Peace protestors ha...</td>\n",
       "      <td>[peace, demo, appeal, reject, peace, protestor...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>Man City 0-2 Man Utd Manchester United reduced...</td>\n",
       "      <td>[man, city, man, utd, manchester, unite, reduc...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>Mobile TV tipped as one to watch Scandinavians...</td>\n",
       "      <td>[mobile, tv, tip, watch, scandinavian, koreans...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>Robots learn 'robotiquette' rules Robots are l...</td>\n",
       "      <td>[robot, learn, robotiquette, rule, robots, lea...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1557 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     French boss to leave EADS The French co-head o...   \n",
       "1     Gamers could drive high-definition TV, films, ...   \n",
       "2     Stalemate in pension strike talks Talks aimed ...   \n",
       "3     Johnny and Denise lose Passport Johnny Vaughan...   \n",
       "4     Tautou 'to star in Da Vinci film' French actre...   \n",
       "5     Media seek Jackson 'juror' notes Reporters cov...   \n",
       "6     Horror film heads US box office A low-budget h...   \n",
       "7     Kerr frustrated at victory margin Republic of ...   \n",
       "8     US casino 'tricks' face ban in UK Controversia...   \n",
       "9     Klinsmann issues Lehmann warning Germany coach...   \n",
       "10    LSE 'sets date for takeover deal' The London S...   \n",
       "11    Councils 'must find Gypsy sites' Ministers are...   \n",
       "12    Low-budget film wins Cesar A film that follows...   \n",
       "13    Women in film 'are earning less' Women in the ...   \n",
       "14    Falconer rebuts 'charade' claims Concessions o...   \n",
       "15    Toxic web links help virus spread Virus writer...   \n",
       "16    FA probes crowd trouble The FA is to take acti...   \n",
       "17    India's rupee hits five-year high India's rupe...   \n",
       "18    California sets fines for spyware The makers o...   \n",
       "19    Defiant hunts put ban to the test Thousands of...   \n",
       "20    Bristol City 2-1 Milton Keynes Leroy Lita took...   \n",
       "21    Smart search lets art fans browse If you don't...   \n",
       "22    Michael film signals 'retirement' Singer Georg...   \n",
       "23    Kilroy-Silk attacked with slurry Euro MP Rober...   \n",
       "24    Sundance to honour foreign films International...   \n",
       "25    Wolves appoint Hoddle as manager Glenn Hoddle ...   \n",
       "26    Benitez deflects blame from Dudek Liverpool ma...   \n",
       "27    Giant waves damage S Asia economy Governments,...   \n",
       "28    Dutch watch Van Gogh's last film The last film...   \n",
       "29    GB quartet get cross country call Four British...   \n",
       "...                                                 ...   \n",
       "1527  Labour attacked on Howard poster Labour has be...   \n",
       "1528  BBC to pour Â£9m into new comedy The BBC is to...   \n",
       "1529  Be careful how you code A new European directi...   \n",
       "1530  US actor Ossie Davis found dead US actor Ossie...   \n",
       "1531  Doors open at biggest gadget fair Thousands of...   \n",
       "1532  German business confidence slides German busin...   \n",
       "1533  Wenger rules out new keeper Arsenal boss Arsen...   \n",
       "1534  The year search became personal The odds are t...   \n",
       "1535  Robinson answers critics England captain Jason...   \n",
       "1536  Federer breezes into semi-finals Roger Federer...   \n",
       "1537  Blair dismisses quit claim report Tony Blair h...   \n",
       "1538  India's Reliance family feud heats up The ongo...   \n",
       "1539  Mixed reaction to Man Utd offer Shares in Manc...   \n",
       "1540  Berlin hails European cinema Organisers say th...   \n",
       "1541  Roundabout continues nostalgia trip The new bi...   \n",
       "1542  Holmes starts 2005 with GB events Kelly Holmes...   \n",
       "1543  Downing injury mars Uefa victory Middlesbrough...   \n",
       "1544  More reforms ahead says Milburn Labour will co...   \n",
       "1545  Two Nigerian banks set to merge Nigerian banks...   \n",
       "1546  Career honour for actor DiCaprio Actor Leonard...   \n",
       "1547  Virus poses as Christmas e-mail Security firms...   \n",
       "1548  Abbas 'will not tolerate' attacks Palestinian ...   \n",
       "1549  Wi-fi web reaches farmers in Peru A network of...   \n",
       "1550  Business fears over sluggish EU economy As Eur...   \n",
       "1551  Tory candidate quits over remark A Conservativ...   \n",
       "1552  Millions to miss out on the net By 2025, 40% o...   \n",
       "1553  Peace demo appeal rejected Peace protestors ha...   \n",
       "1554  Man City 0-2 Man Utd Manchester United reduced...   \n",
       "1555  Mobile TV tipped as one to watch Scandinavians...   \n",
       "1556  Robots learn 'robotiquette' rules Robots are l...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [french, bos, leave, eads, french, head, europ...   \n",
       "1     [gamers, drive, high, definition, tv, film, ga...   \n",
       "2     [stalemate, pension, strike, talk, talk, aim, ...   \n",
       "3     [johnny, denise, lose, passport, johnny, vaugh...   \n",
       "4     [tautou, star, da, vinci, film, french, actres...   \n",
       "5     [medium, seek, jackson, juror, note, reporter,...   \n",
       "6     [horror, film, head, box, office, low, budget,...   \n",
       "7     [kerr, frustrate, victory, margin, republic, i...   \n",
       "8     [casino, trick, face, ban, uk, controversial, ...   \n",
       "9     [klinsmann, issue, lehmann, warn, germany, coa...   \n",
       "10    [lse, set, date, takeover, deal, london, stock...   \n",
       "11    [council, find, gypsy, site, minister, tell, c...   \n",
       "12    [low, budget, film, win, cesar, film, group, a...   \n",
       "13    [woman, film, earn, woman, uk, film, industry,...   \n",
       "14    [falconer, rebuts, charade, claim, concession,...   \n",
       "15    [toxic, web, link, virus, spread, virus, write...   \n",
       "16    [fa, probe, crowd, trouble, fa, action, troubl...   \n",
       "17    [india, rupee, hit, year, high, india, rupee, ...   \n",
       "18    [california, set, fine, spyware, maker, comput...   \n",
       "19    [defiant, hunt, put, ban, test, thousand, hunt...   \n",
       "20    [bristol, city, milton, keynes, leroy, lita, g...   \n",
       "21    [smart, search, let, art, fan, browse, art, se...   \n",
       "22    [michael, film, signal, retirement, singer, ge...   \n",
       "23    [kilroy, silk, attack, slurry, euro, mp, rober...   \n",
       "24    [sundance, honour, foreign, film, internationa...   \n",
       "25    [wolf, appoint, hoddle, manager, glenn, hoddle...   \n",
       "26    [benitez, deflects, blame, dudek, liverpool, m...   \n",
       "27    [giant, wave, damage, asia, economy, governmen...   \n",
       "28    [dutch, watch, van, gogh, film, film, make, sl...   \n",
       "29    [gb, quartet, cross, country, call, british, a...   \n",
       "...                                                 ...   \n",
       "1527  [labour, attack, howard, poster, labour, accus...   \n",
       "1528  [bbc, pour, â, 9m, comedy, bbc, invest, â, 9m,...   \n",
       "1529  [careful, code, european, directive, put, soft...   \n",
       "1530  [actor, ossie, davis, find, dead, actor, ossie...   \n",
       "1531  [door, open, big, gadget, fair, thousand, tech...   \n",
       "1532  [german, business, confidence, slides, german,...   \n",
       "1533  [wenger, rule, keeper, arsenal, bos, arsene, w...   \n",
       "1534  [year, search, personal, odds, fire, browser, ...   \n",
       "1535  [robinson, answer, critic, england, captain, j...   \n",
       "1536  [federer, breeze, semi, final, roger, federer,...   \n",
       "1537  [blair, dismiss, quit, claim, report, tony, bl...   \n",
       "1538  [india, reliance, family, feud, heat, ongoing,...   \n",
       "1539  [mixed, reaction, man, utd, offer, share, manc...   \n",
       "1540  [berlin, hail, european, cinema, organiser, ye...   \n",
       "1541  [roundabout, continue, nostalgia, trip, big, s...   \n",
       "1542  [holmes, start, gb, event, kelly, holmes, star...   \n",
       "1543  [down, injury, mar, uefa, victory, middlesbrou...   \n",
       "1544  [reform, ahead, milburn, labour, continue, pur...   \n",
       "1545  [nigerian, bank, set, merge, nigerian, bank, u...   \n",
       "1546  [career, honour, actor, dicaprio, actor, leona...   \n",
       "1547  [virus, pose, christmas, mail, security, firm,...   \n",
       "1548  [abbas, tolerate, attack, palestinian, leader,...   \n",
       "1549  [wi, fi, web, reach, farmer, peru, network, co...   \n",
       "1550  [business, fear, sluggish, eu, economy, europe...   \n",
       "1551  [tory, candidate, quits, remark, conservative,...   \n",
       "1552  [million, miss, net, uk, population, internet,...   \n",
       "1553  [peace, demo, appeal, reject, peace, protestor...   \n",
       "1554  [man, city, man, utd, manchester, unite, reduc...   \n",
       "1555  [mobile, tv, tip, watch, scandinavian, koreans...   \n",
       "1556  [robot, learn, robotiquette, rule, robots, lea...   \n",
       "\n",
       "                                                  tfidf       category  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "5     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "6     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "7     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "8     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "9     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "10    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "11    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "12    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "13    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "14    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "15    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "16    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "17    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "18    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "19    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "20    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "21    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "22    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "23    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "24    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "25    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "26    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "27    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "28    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "29    [0.011810634137970098, 0.0, 0.0, 0.0, 0.0, 0.0...          sport  \n",
       "...                                                 ...            ...  \n",
       "1527  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "1528  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "1529  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1530  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "1531  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1532  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1533  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "1534  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1535  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "1536  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "1537  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "1538  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1539  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1540  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "1541  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "1542  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "1543  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "1544  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "1545  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1546  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  entertainment  \n",
       "1547  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1548  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "1549  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1550  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       business  \n",
       "1551  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "1552  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1553  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       politics  \n",
       "1554  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...          sport  \n",
       "1555  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "1556  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           tech  \n",
       "\n",
       "[1557 rows x 4 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle.to_csv('./tfidf_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_handle, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1043\n",
      "Test: 514\n"
     ]
    }
   ],
   "source": [
    "print('Train: ' + repr(len(train)))\n",
    "print('Test: ' + repr(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(value1, value2):\n",
    "    return np.linalg.norm(value1-value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours(vector):\n",
    "    ret = []\n",
    "    for index, row in train.iterrows():\n",
    "        ret.append([row.category, euclideanDistance(row['tfidf'], vector)])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content     BAA support ahead of court battle UK airport o...\n",
       "tokens      [baa, support, ahead, court, battle, uk, airpo...\n",
       "tfidf       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "category                                             politics\n",
       "Name: 584, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, ..., 3, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_Labels = le.fit_transform(df_handle['category'].tolist())\n",
    "encoded_Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer(tokenizer=tokenize)\n",
    "val = tfidf_transformer.fit_transform(df_handle['content'])\n",
    "X_train_data, X_test_data, y_train_labels, y_test_labels = train_test_split(val, encoded_Labels, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 0 1 1 3 0 1 1 4 3 3 4 2 3 1 2 4 3 0 0 4 2 4 3 4 1 2 0 4 2 3 2 2 0 1 0\n",
      " 0 3 3 3 1 0 4 1 0 1 3 1 0 2 4 4 4 2 3 4 0 2 0 1 2 4 4 2 2 0 0 0 4 4 4 3 3\n",
      " 1 4 1 4 2 2 1 2 1 4 3 2 2 2 0 0 2 4 2 3 3 2 0 2 0 2 1 1 0 1 3 0 1 3 4 4 4\n",
      " 2 0 2 0 2 2 2 3 4 3 4 0 3 2 4 1 2 3 3 3 4 3 3 3 3 4 1 1 1 1 1 4 4 2 2 3 3\n",
      " 3 4 1 4 1 4 3 0 3 2 2 4 0 0 3 2 1 4 4 3 0 2 2 3 4 0 2 2 3 0 3 4 1 4 2 0 3\n",
      " 3 2 2 0 0 3 1 3 4 2 3 2 4 0 3 4 0 2 4 3 1 2 2 3 0 3 2 3 0 3 1 4 1 3 0 1 3\n",
      " 3 2 1 2 4 4 2 3 2 1 3 4 0 1 1 0 2 3 4 4 3 3 4 2 3 0 4 3 2 4 1 4 1 2 2 4 0\n",
      " 0 1 2 0 0 3 2 1 2 1 0 0 1 3 1 0 2 2 4 0 2 3 0 4 4 4 4 2 0 4 2 3 1 4 0 4 3\n",
      " 0 2 2 3 3 4 2 3 3 2 2 1 0 4 4 0 3 2 0 3 2 3 0 3 3 0 1 4 3 4 0 1 0 2 1 1 3\n",
      " 3 3 3 4 3 3 2 3 0 4 2 4 0 0 1 0 3 3 4 1 3 0 0 4 1 1 4 0 1 0 3 3 0 0 2 4 1\n",
      " 4 2 1 4 0 2 4 3 0 0 3 1 3 3 0 1 2 0 4 2 3 0 2 4 4 3 3 1 1 2 4 4 2 3 2 4 1\n",
      " 4 3 3 4 3 4 1 3 4 4 0 0 0 0 4 0 3 2 3 2 0 4 3 3 3 0 3 4 1 0 1 2 4 1 0 2 4\n",
      " 1 1 3 4 0 4 3 3 0 4 3 3 1 0 0 2 1 0 4 0 4 0 0 0 4 4 3 2 3 4 3 1 1 3 2 0 3\n",
      " 0 3 0 4 0 2 2 3 0 0 3 1 4 2 1 0 1 2 2 3 3 0 2 0 2 2 0 2 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train_data, y_train_labels)\n",
    "\n",
    "#Train the model using the training sets\n",
    "# knn.fit(train_input_vectors, train_labels)\n",
    "# t_input_vectors = tfidf_transformer.fit_transform(train.content)\n",
    "\n",
    "\n",
    "# test_input_vectors = tfidf_transformer.transform(test.content)\n",
    "# print(train_input_vectors.shape)\n",
    "# print(test_input_vectors.shape)\n",
    "# test_input_vectors.shape\n",
    "#Predict the response for test dataset\n",
    "y_label_pred = knn.predict(X_test_data)\n",
    "print(y_label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9105058365758755\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_labels, y_label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model KNN (k=5): 0.9105058365758755\n"
     ]
    }
   ],
   "source": [
    "# comparing this accuracy metric format with the one above\n",
    "predicted_KNN2 = KNeighborsClassifier(n_neighbors=5).fit(X_train_data, y_train_labels).predict(X_test_data)\n",
    "print('Accuracy for model KNN (k=5): '+ str(np.mean(predicted_KNN2 == y_test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "tp = 0\n",
    "sample_size = 5\n",
    "\n",
    "for i in range(len(test)):\n",
    "    nearest_neighbours = get_nearest_neighbours(test.iloc[i]['tfidf'])\n",
    "    sort_NN = list(sorted(nearest_neighbours, key=lambda x: x[1], reverse=True)) # sort the returned list of vectors in order of highest to loweest distance\n",
    "    \n",
    "    k=10\n",
    "    votes = defaultdict(int) # create dictionary of votes and tallied votes\n",
    "    for j in range(k):\n",
    "        votes[sort_NN[j][0]] += 1\n",
    "    final_vote = list(sorted(votes.items(), key=itemgetter(1), reverse=True ))[0][0] # put highest voted value first\n",
    "    tp += int(final_vote == test.iloc[j]['category'])\n",
    "    \n",
    "accuracy = tp / len(test)\n",
    "    # logic for choosing what got voted for\n",
    "    # if category voted for equals best label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "sport\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(final_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Cross-Validation Using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of tokenization: custom tokenizer vs built in tokenizer.\n",
    "Evaluated below on native bayes model\n",
    "Initial results: custom tokenizer is better (~95-96.5% accuracy) vs the standard one (~92%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96638655 0.95798319 0.95798319 0.97478992 0.96581197 0.96551724\n",
      " 0.95689655 0.95614035 0.95614035 0.97368421]\n",
      "0.9631333528338972\n"
     ]
    }
   ],
   "source": [
    "# testing simple native bayes with custom tokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_handle)\n",
    "tfidf_transformer = TfidfVectorizer(tokenizer=tokenize)\n",
    "train_input_vectors = tfidf_transformer.fit_transform(train.content)\n",
    "train_labels = train.category\n",
    "scores = cross_val_score(MultinomialNB(), train_input_vectors, train_labels, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91525424 0.92372881 0.93220339 0.94067797 0.93162393 0.93965517\n",
      " 0.93103448 0.92241379 0.9137931  0.95614035]\n",
      "0.9306525241004925\n"
     ]
    }
   ],
   "source": [
    "# testing simple naive bayes with standard built in tokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_handle)\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "train_input_vectors = tfidf_transformer.fit_transform(train.content)\n",
    "train_labels = train.category\n",
    "scores = cross_val_score(MultinomialNB(), train_input_vectors, train_labels, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(train_input_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\n",
      "French boss to leave EADS The French co-head of European defence and aerospace group EADS Philippe Camus is to leave his post\n",
      "Model prediction: business\n",
      "\n",
      "Article 1:\n",
      "Gamers could drive high-definition TV, films, and games have been gearing up for some time now for the next revolution to transform the quality of what is on our screens\n",
      "Model prediction: business\n",
      "\n",
      "Article 2:\n",
      "Stalemate in pension strike talks Talks aimed at averting national strikes over pension reforms have ended without agreement after 90 minutes\n",
      "Model prediction: politics\n",
      "\n",
      "Article 3:\n",
      "Johnny and Denise lose Passport Johnny Vaughan and Denise Van Outen's Saturday night entertainment show Passport to Paradise will not return to screens, the BBC has said\n",
      "Model prediction: business\n",
      "\n",
      "Article 4:\n",
      "Tautou 'to star in Da Vinci film' French actress Audrey Tautou, star of hit film Amelie, will play the female lead in the film adaptation of The Da Vinci Code, it has been reported\n",
      "Model prediction: sport\n",
      "\n",
      "Article 5:\n",
      "Media seek Jackson 'juror' notes Reporters covering singer Michael Jackson's trial in California have asked to see questionnaires completed by potential jurors\n",
      "Model prediction: entertainment\n",
      "\n",
      "Article 6:\n",
      "Horror film heads US box office A low-budget horror film produced by Evil Dead director Sam Raimi has topped the North American box office\n",
      "Model prediction: sport\n",
      "\n",
      "Article 7:\n",
      "Kerr frustrated at victory margin Republic of Ireland manager Brian Kerr admitted he was frustrated his side did not score more than one goal in their friendly win over Croatia\n",
      "Model prediction: sport\n",
      "\n",
      "Article 8:\n",
      "US casino 'tricks' face ban in UK Controversial new UK casinos will be banned from using American tricks of the trade to ensure they are \"socially responsible\", it has been suggested\n",
      "Model prediction: politics\n",
      "\n",
      "Article 9:\n",
      "Klinsmann issues Lehmann warning Germany coach Jurgen Klinsmann has warned goalkeeper Jens Lehmann he may have to quit Arsenal to keep his World Cup dreams alive\n",
      "Model prediction: tech\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Article %s:\" % i)\n",
    "    print(df_handle.content[i].split('.')[0])\n",
    "    print(\"Model prediction: %s\" % model.predict(train_input_vectors[i])[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## More Sklearn Models\n",
    "\n",
    "Try out more models from Sklearn and report their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolu_\\Anaconda3\\envs\\comp30810py36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes average accuracy (10-fold x-val): 0.931568\n",
      "Nearest Neighbors average accuracy (10-fold x-val): 0.881047\n",
      "AdaBoost average accuracy (10-fold x-val): 0.735205\n",
      "Linear SVM average accuracy (10-fold x-val): 0.236496\n",
      "RBF SVM average accuracy (10-fold x-val): 0.932365\n",
      "Decision Tree average accuracy (10-fold x-val): 0.658010\n",
      "Random Forest average accuracy (10-fold x-val): 0.315390\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = [\n",
    "    \"Multinomial Naive Bayes\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"AdaBoost\",\n",
    "    \"Linear SVM\", \n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",  \n",
    "#     \"Naive Bayes\",\n",
    "#     \"Neural Net\",\n",
    "#     \"Gaussian Process\",\n",
    "         ]\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(alpha=1.0),\n",
    "    KNeighborsClassifier(3),\n",
    "    AdaBoostClassifier(),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1), # took to long to run\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)), # needs a 'dense matrix'?\n",
    "#     GaussianNB(), # also needs a 'dense matrix'?\n",
    "]\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(classifier, train_input_vectors, train_labels, cv=k_folds)\n",
    "    average_accuracy = np.mean(scores)\n",
    "\n",
    "    print(\"%s average accuracy (%d-fold x-val): %f\" \n",
    "          % (name, k_folds, average_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "```\n",
    "Multinomial Naive Bayes average accuracy (10-fold x-val): 0.950307\n",
    "Nearest Neighbors average accuracy (10-fold x-val): 0.892863\n",
    "AdaBoost average accuracy (10-fold x-val): 0.712065\n",
    "Linear SVM average accuracy (10-fold x-val): 0.224502\n",
    "RBF SVM average accuracy (10-fold x-val): 0.944214\n",
    "Decision Tree average accuracy (10-fold x-val): 0.653080\n",
    "Random Forest average accuracy (10-fold x-val): 0.323166\n",
    "```\n",
    "\n",
    "The Naive Bayes and RBF SVM are by far the most-promising. We should look into tweaking these models further to see if we can improve on the results.\n",
    "\n",
    "The Nearest Neighbour model also performs well. We check to see which value of k provides the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (k=1) average accuracy (10-fold x-val): 0.885187\n",
      "KNN (k=2) average accuracy (10-fold x-val): 0.873316\n",
      "KNN (k=3) average accuracy (10-fold x-val): 0.881047\n",
      "KNN (k=4) average accuracy (10-fold x-val): 0.880169\n",
      "KNN (k=5) average accuracy (10-fold x-val): 0.885179\n",
      "KNN (k=6) average accuracy (10-fold x-val): 0.889513\n",
      "KNN (k=7) average accuracy (10-fold x-val): 0.880943\n",
      "KNN (k=8) average accuracy (10-fold x-val): 0.881879\n",
      "KNN (k=9) average accuracy (10-fold x-val): 0.878394\n",
      "KNN (k=10) average accuracy (10-fold x-val): 0.878415\n",
      "KNN (k=11) average accuracy (10-fold x-val): 0.875896\n",
      "KNN (k=12) average accuracy (10-fold x-val): 0.879337\n",
      "KNN (k=13) average accuracy (10-fold x-val): 0.871606\n",
      "KNN (k=14) average accuracy (10-fold x-val): 0.875018\n",
      "KNN (k=15) average accuracy (10-fold x-val): 0.873250\n",
      "KNN (k=16) average accuracy (10-fold x-val): 0.871547\n",
      "KNN (k=17) average accuracy (10-fold x-val): 0.871510\n",
      "KNN (k=18) average accuracy (10-fold x-val): 0.872387\n",
      "KNN (k=19) average accuracy (10-fold x-val): 0.869801\n",
      "KNN (k=20) average accuracy (10-fold x-val): 0.869830\n",
      "KNN (k=21) average accuracy (10-fold x-val): 0.864657\n",
      "KNN (k=22) average accuracy (10-fold x-val): 0.862962\n",
      "KNN (k=23) average accuracy (10-fold x-val): 0.858717\n",
      "KNN (k=24) average accuracy (10-fold x-val): 0.858622\n",
      "KNN (k=25) average accuracy (10-fold x-val): 0.861201\n",
      "KNN (k=26) average accuracy (10-fold x-val): 0.858578\n",
      "KNN (k=27) average accuracy (10-fold x-val): 0.861172\n",
      "KNN (k=28) average accuracy (10-fold x-val): 0.861127\n",
      "KNN (k=29) average accuracy (10-fold x-val): 0.861127\n",
      "KNN (k=30) average accuracy (10-fold x-val): 0.861127\n",
      "KNN (k=31) average accuracy (10-fold x-val): 0.857745\n",
      "KNN (k=32) average accuracy (10-fold x-val): 0.857753\n",
      "KNN (k=33) average accuracy (10-fold x-val): 0.858563\n",
      "KNN (k=34) average accuracy (10-fold x-val): 0.853464\n",
      "KNN (k=35) average accuracy (10-fold x-val): 0.854370\n",
      "KNN (k=36) average accuracy (10-fold x-val): 0.855189\n",
      "KNN (k=37) average accuracy (10-fold x-val): 0.852594\n",
      "KNN (k=38) average accuracy (10-fold x-val): 0.855181\n",
      "KNN (k=39) average accuracy (10-fold x-val): 0.852601\n",
      "KNN (k=40) average accuracy (10-fold x-val): 0.855151\n",
      "KNN (k=41) average accuracy (10-fold x-val): 0.853455\n",
      "KNN (k=42) average accuracy (10-fold x-val): 0.852586\n",
      "KNN (k=43) average accuracy (10-fold x-val): 0.851717\n",
      "KNN (k=44) average accuracy (10-fold x-val): 0.851724\n",
      "KNN (k=45) average accuracy (10-fold x-val): 0.850044\n",
      "KNN (k=46) average accuracy (10-fold x-val): 0.845792\n",
      "KNN (k=47) average accuracy (10-fold x-val): 0.844900\n",
      "KNN (k=48) average accuracy (10-fold x-val): 0.844045\n",
      "KNN (k=49) average accuracy (10-fold x-val): 0.844893\n",
      "KNN (k=50) average accuracy (10-fold x-val): 0.844907\n",
      "KNN (k=51) average accuracy (10-fold x-val): 0.844053\n",
      "KNN (k=52) average accuracy (10-fold x-val): 0.843220\n",
      "KNN (k=53) average accuracy (10-fold x-val): 0.844944\n",
      "KNN (k=54) average accuracy (10-fold x-val): 0.843227\n",
      "KNN (k=55) average accuracy (10-fold x-val): 0.838046\n",
      "KNN (k=56) average accuracy (10-fold x-val): 0.836322\n",
      "KNN (k=57) average accuracy (10-fold x-val): 0.835489\n",
      "KNN (k=58) average accuracy (10-fold x-val): 0.835496\n",
      "KNN (k=59) average accuracy (10-fold x-val): 0.831215\n",
      "KNN (k=60) average accuracy (10-fold x-val): 0.832077\n",
      "KNN (k=61) average accuracy (10-fold x-val): 0.829506\n",
      "KNN (k=62) average accuracy (10-fold x-val): 0.828673\n",
      "KNN (k=63) average accuracy (10-fold x-val): 0.829469\n",
      "KNN (k=64) average accuracy (10-fold x-val): 0.830353\n",
      "KNN (k=65) average accuracy (10-fold x-val): 0.828659\n",
      "KNN (k=66) average accuracy (10-fold x-val): 0.827774\n",
      "KNN (k=67) average accuracy (10-fold x-val): 0.825261\n",
      "KNN (k=68) average accuracy (10-fold x-val): 0.823537\n",
      "KNN (k=69) average accuracy (10-fold x-val): 0.822689\n",
      "KNN (k=70) average accuracy (10-fold x-val): 0.826124\n",
      "KNN (k=71) average accuracy (10-fold x-val): 0.819271\n",
      "KNN (k=72) average accuracy (10-fold x-val): 0.820980\n",
      "KNN (k=73) average accuracy (10-fold x-val): 0.822697\n",
      "KNN (k=74) average accuracy (10-fold x-val): 0.820125\n",
      "KNN (k=75) average accuracy (10-fold x-val): 0.820088\n",
      "KNN (k=76) average accuracy (10-fold x-val): 0.819278\n",
      "KNN (k=77) average accuracy (10-fold x-val): 0.820111\n",
      "KNN (k=78) average accuracy (10-fold x-val): 0.820133\n",
      "KNN (k=79) average accuracy (10-fold x-val): 0.817487\n",
      "KNN (k=80) average accuracy (10-fold x-val): 0.815763\n",
      "KNN (k=81) average accuracy (10-fold x-val): 0.814922\n",
      "KNN (k=82) average accuracy (10-fold x-val): 0.814951\n",
      "KNN (k=83) average accuracy (10-fold x-val): 0.813242\n",
      "KNN (k=84) average accuracy (10-fold x-val): 0.813227\n",
      "KNN (k=85) average accuracy (10-fold x-val): 0.815799\n",
      "KNN (k=86) average accuracy (10-fold x-val): 0.812343\n",
      "KNN (k=87) average accuracy (10-fold x-val): 0.813212\n",
      "KNN (k=88) average accuracy (10-fold x-val): 0.810640\n",
      "KNN (k=89) average accuracy (10-fold x-val): 0.811517\n",
      "KNN (k=90) average accuracy (10-fold x-val): 0.811525\n",
      "KNN (k=91) average accuracy (10-fold x-val): 0.814104\n",
      "KNN (k=92) average accuracy (10-fold x-val): 0.812394\n",
      "KNN (k=93) average accuracy (10-fold x-val): 0.810677\n",
      "KNN (k=94) average accuracy (10-fold x-val): 0.811532\n",
      "KNN (k=95) average accuracy (10-fold x-val): 0.813264\n",
      "KNN (k=96) average accuracy (10-fold x-val): 0.810677\n",
      "KNN (k=97) average accuracy (10-fold x-val): 0.810684\n",
      "KNN (k=98) average accuracy (10-fold x-val): 0.810662\n",
      "KNN (k=99) average accuracy (10-fold x-val): 0.810640\n",
      "Best k value is 6\n"
     ]
    }
   ],
   "source": [
    "best_k = 0\n",
    "best_k_accuracy = 0\n",
    "\n",
    "for k in range(1,100):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(KNeighborsClassifier(k), train_input_vectors, train_labels, cv=k_folds)\n",
    "    average_accuracy = np.mean(scores)\n",
    "    \n",
    "    if average_accuracy > best_k_accuracy:\n",
    "        best_k_accuracy = average_accuracy\n",
    "        best_k = k\n",
    "\n",
    "    print(\"KNN (k=%d) average accuracy (%d-fold x-val): %f\" \n",
    "          % (k, k_folds, average_accuracy))\n",
    "    \n",
    "\n",
    "print(\"Best k value is %d\" % best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing KNN uses k=26, accuracy = 0.904047. Still not as good as the NB or RBF SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search range is: [0.001   0.00108 0.00116 0.00124 0.00132 0.0014  0.00148 0.00156 0.00164\n",
      " 0.00172 0.0018  0.00188 0.00196 0.00204 0.00212 0.0022  0.00228 0.00236\n",
      " 0.00244 0.00252 0.0026  0.00268 0.00276 0.00284 0.00292 0.003   0.00308\n",
      " 0.00316 0.00324 0.00332 0.0034  0.00348 0.00356 0.00364 0.00372 0.0038\n",
      " 0.00388 0.00396 0.00404 0.00412 0.0042  0.00428 0.00436 0.00444 0.00452\n",
      " 0.0046  0.00468 0.00476 0.00484 0.00492 0.005   0.00508 0.00516 0.00524\n",
      " 0.00532 0.0054  0.00548 0.00556 0.00564 0.00572 0.0058  0.00588 0.00596\n",
      " 0.00604 0.00612 0.0062  0.00628 0.00636 0.00644 0.00652 0.0066  0.00668\n",
      " 0.00676 0.00684 0.00692 0.007   0.00708 0.00716 0.00724 0.00732 0.0074\n",
      " 0.00748 0.00756 0.00764 0.00772 0.0078  0.00788 0.00796 0.00804 0.00812\n",
      " 0.0082  0.00828 0.00836 0.00844 0.00852 0.0086  0.00868 0.00876 0.00884\n",
      " 0.00892 0.009  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('The search range is: ' + str(np.linspace(0.001,0.009,101)))\n",
    "bayes_clf_val = MultinomialNB()\n",
    "bayes_parameters = {'alpha': (np.linspace(0.001,0.009,101))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter is:  0.001\n"
     ]
    }
   ],
   "source": [
    "bayes_gs_clf = GridSearchCV(bayes_clf_val, param_grid = bayes_parameters, cv=10, scoring = 'accuracy')\n",
    "bayes_gs_clf = bayes_gs_clf.fit(train_input_vectors, train_labels)\n",
    "NB_parameters = bayes_gs_clf.best_params_\n",
    "print('The best parameter is: ', NB_parameters['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolu_\\Anaconda3\\envs\\comp30810py36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.97172, std: 0.00844, params: {'alpha': 0.001},\n",
       " mean: 0.97172, std: 0.00844, params: {'alpha': 0.00108},\n",
       " mean: 0.97172, std: 0.00844, params: {'alpha': 0.00116},\n",
       " mean: 0.97087, std: 0.00935, params: {'alpha': 0.00124},\n",
       " mean: 0.97087, std: 0.00935, params: {'alpha': 0.00132},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0014},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00148},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0015600000000000002},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00164},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0017200000000000002},\n",
       " mean: 0.96915, std: 0.01203, params: {'alpha': 0.0018},\n",
       " mean: 0.97087, std: 0.00935, params: {'alpha': 0.0018800000000000002},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00196},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00204},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0021200000000000004},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0022},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00228},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00236},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0024400000000000003},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00252},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0026},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00268},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0027600000000000003},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00284},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0029200000000000003},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.003},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0030800000000000003},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00316},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0032400000000000003},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00332},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0034000000000000002},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00348},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0035600000000000002},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0036400000000000004},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00372},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.0038000000000000004},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00388},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00396},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00404},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00412},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.004200000000000001},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.004280000000000001},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00436},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00444},\n",
       " mean: 0.97001, std: 0.01083, params: {'alpha': 0.00452},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0046},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00468},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00476},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0048400000000000006},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.004920000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.005},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00508},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0051600000000000005},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.005240000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00532},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0054},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0054800000000000005},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.005560000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00564},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00572},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0058000000000000005},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.005880000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00596},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00604},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0061200000000000004},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.006200000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.006280000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00636},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00644},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.006520000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.006600000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00668},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00676},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.006840000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.006920000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.007},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00708},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0071600000000000006},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.007240000000000001},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.00732},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0074},\n",
       " mean: 0.96915, std: 0.01011, params: {'alpha': 0.0074800000000000005},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.007560000000000001},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.007640000000000001},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00772},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.0078000000000000005},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.007880000000000002},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.007960000000000002},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00804},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00812},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.0082},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.008280000000000001},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00836},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00844},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00852},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.0086},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00868},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00876},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00884},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.00892},\n",
       " mean: 0.96829, std: 0.00998, params: {'alpha': 0.009}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search range is: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n"
     ]
    }
   ],
   "source": [
    "print('The search range is: ' + str(np.arange(1,100,1)))\n",
    "knn_clf_val = KNeighborsClassifier()\n",
    "knn_parameters = {'n_neighbors': (np.arange(1,100,1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter is:  6\n"
     ]
    }
   ],
   "source": [
    "knn_gs_clf = GridSearchCV(knn_clf_val, param_grid = knn_parameters, cv=10, scoring = 'accuracy')\n",
    "knn_gs_clf = knn_gs_clf.fit(train_input_vectors, train_labels)\n",
    "KNN_parameters = knn_gs_clf.best_params_\n",
    "print('The best parameter is: ', KNN_parameters['n_neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolu_\\Anaconda3\\envs\\comp30810py36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.88518, std: 0.01905, params: {'n_neighbors': 1},\n",
       " mean: 0.87318, std: 0.03244, params: {'n_neighbors': 2},\n",
       " mean: 0.88089, std: 0.03046, params: {'n_neighbors': 3},\n",
       " mean: 0.88003, std: 0.02259, params: {'n_neighbors': 4},\n",
       " mean: 0.88518, std: 0.01431, params: {'n_neighbors': 5},\n",
       " mean: 0.88946, std: 0.02334, params: {'n_neighbors': 6},\n",
       " mean: 0.88089, std: 0.02042, params: {'n_neighbors': 7},\n",
       " mean: 0.88175, std: 0.02317, params: {'n_neighbors': 8},\n",
       " mean: 0.87832, std: 0.02654, params: {'n_neighbors': 9},\n",
       " mean: 0.87832, std: 0.02071, params: {'n_neighbors': 10},\n",
       " mean: 0.87575, std: 0.02385, params: {'n_neighbors': 11},\n",
       " mean: 0.87918, std: 0.02064, params: {'n_neighbors': 12},\n",
       " mean: 0.87147, std: 0.02247, params: {'n_neighbors': 13},\n",
       " mean: 0.87489, std: 0.01914, params: {'n_neighbors': 14},\n",
       " mean: 0.87318, std: 0.02319, params: {'n_neighbors': 15},\n",
       " mean: 0.87147, std: 0.02286, params: {'n_neighbors': 16},\n",
       " mean: 0.87147, std: 0.02408, params: {'n_neighbors': 17},\n",
       " mean: 0.87232, std: 0.02281, params: {'n_neighbors': 18},\n",
       " mean: 0.86975, std: 0.02309, params: {'n_neighbors': 19},\n",
       " mean: 0.86975, std: 0.02303, params: {'n_neighbors': 20},\n",
       " mean: 0.86461, std: 0.02189, params: {'n_neighbors': 21},\n",
       " mean: 0.86290, std: 0.02320, params: {'n_neighbors': 22},\n",
       " mean: 0.85861, std: 0.02328, params: {'n_neighbors': 23},\n",
       " mean: 0.85861, std: 0.02476, params: {'n_neighbors': 24},\n",
       " mean: 0.86118, std: 0.02300, params: {'n_neighbors': 25},\n",
       " mean: 0.85861, std: 0.02744, params: {'n_neighbors': 26},\n",
       " mean: 0.86118, std: 0.02395, params: {'n_neighbors': 27},\n",
       " mean: 0.86118, std: 0.02752, params: {'n_neighbors': 28},\n",
       " mean: 0.86118, std: 0.02984, params: {'n_neighbors': 29},\n",
       " mean: 0.86118, std: 0.02905, params: {'n_neighbors': 30},\n",
       " mean: 0.85775, std: 0.02715, params: {'n_neighbors': 31},\n",
       " mean: 0.85775, std: 0.02792, params: {'n_neighbors': 32},\n",
       " mean: 0.85861, std: 0.02756, params: {'n_neighbors': 33},\n",
       " mean: 0.85347, std: 0.03125, params: {'n_neighbors': 34},\n",
       " mean: 0.85433, std: 0.02881, params: {'n_neighbors': 35},\n",
       " mean: 0.85518, std: 0.03090, params: {'n_neighbors': 36},\n",
       " mean: 0.85261, std: 0.03309, params: {'n_neighbors': 37},\n",
       " mean: 0.85518, std: 0.02917, params: {'n_neighbors': 38},\n",
       " mean: 0.85261, std: 0.02800, params: {'n_neighbors': 39},\n",
       " mean: 0.85518, std: 0.02558, params: {'n_neighbors': 40},\n",
       " mean: 0.85347, std: 0.02750, params: {'n_neighbors': 41},\n",
       " mean: 0.85261, std: 0.02923, params: {'n_neighbors': 42},\n",
       " mean: 0.85176, std: 0.03134, params: {'n_neighbors': 43},\n",
       " mean: 0.85176, std: 0.03154, params: {'n_neighbors': 44},\n",
       " mean: 0.85004, std: 0.02868, params: {'n_neighbors': 45},\n",
       " mean: 0.84576, std: 0.02956, params: {'n_neighbors': 46},\n",
       " mean: 0.84490, std: 0.02489, params: {'n_neighbors': 47},\n",
       " mean: 0.84404, std: 0.02960, params: {'n_neighbors': 48},\n",
       " mean: 0.84490, std: 0.02906, params: {'n_neighbors': 49},\n",
       " mean: 0.84490, std: 0.02823, params: {'n_neighbors': 50},\n",
       " mean: 0.84404, std: 0.02856, params: {'n_neighbors': 51},\n",
       " mean: 0.84319, std: 0.02720, params: {'n_neighbors': 52},\n",
       " mean: 0.84490, std: 0.02907, params: {'n_neighbors': 53},\n",
       " mean: 0.84319, std: 0.02847, params: {'n_neighbors': 54},\n",
       " mean: 0.83805, std: 0.03288, params: {'n_neighbors': 55},\n",
       " mean: 0.83633, std: 0.03153, params: {'n_neighbors': 56},\n",
       " mean: 0.83548, std: 0.03072, params: {'n_neighbors': 57},\n",
       " mean: 0.83548, std: 0.03054, params: {'n_neighbors': 58},\n",
       " mean: 0.83119, std: 0.02884, params: {'n_neighbors': 59},\n",
       " mean: 0.83205, std: 0.02835, params: {'n_neighbors': 60},\n",
       " mean: 0.82948, std: 0.03062, params: {'n_neighbors': 61},\n",
       " mean: 0.82862, std: 0.02757, params: {'n_neighbors': 62},\n",
       " mean: 0.82948, std: 0.02915, params: {'n_neighbors': 63},\n",
       " mean: 0.83033, std: 0.02544, params: {'n_neighbors': 64},\n",
       " mean: 0.82862, std: 0.02632, params: {'n_neighbors': 65},\n",
       " mean: 0.82776, std: 0.02540, params: {'n_neighbors': 66},\n",
       " mean: 0.82519, std: 0.02961, params: {'n_neighbors': 67},\n",
       " mean: 0.82348, std: 0.02783, params: {'n_neighbors': 68},\n",
       " mean: 0.82262, std: 0.02975, params: {'n_neighbors': 69},\n",
       " mean: 0.82605, std: 0.03043, params: {'n_neighbors': 70},\n",
       " mean: 0.81919, std: 0.03572, params: {'n_neighbors': 71},\n",
       " mean: 0.82091, std: 0.02872, params: {'n_neighbors': 72},\n",
       " mean: 0.82262, std: 0.03013, params: {'n_neighbors': 73},\n",
       " mean: 0.82005, std: 0.02750, params: {'n_neighbors': 74},\n",
       " mean: 0.82005, std: 0.02702, params: {'n_neighbors': 75},\n",
       " mean: 0.81919, std: 0.02938, params: {'n_neighbors': 76},\n",
       " mean: 0.82005, std: 0.02792, params: {'n_neighbors': 77},\n",
       " mean: 0.82005, std: 0.02773, params: {'n_neighbors': 78},\n",
       " mean: 0.81748, std: 0.02869, params: {'n_neighbors': 79},\n",
       " mean: 0.81577, std: 0.03031, params: {'n_neighbors': 80},\n",
       " mean: 0.81491, std: 0.02824, params: {'n_neighbors': 81},\n",
       " mean: 0.81491, std: 0.02613, params: {'n_neighbors': 82},\n",
       " mean: 0.81320, std: 0.02543, params: {'n_neighbors': 83},\n",
       " mean: 0.81320, std: 0.02369, params: {'n_neighbors': 84},\n",
       " mean: 0.81577, std: 0.02323, params: {'n_neighbors': 85},\n",
       " mean: 0.81234, std: 0.02652, params: {'n_neighbors': 86},\n",
       " mean: 0.81320, std: 0.02527, params: {'n_neighbors': 87},\n",
       " mean: 0.81063, std: 0.02552, params: {'n_neighbors': 88},\n",
       " mean: 0.81148, std: 0.02666, params: {'n_neighbors': 89},\n",
       " mean: 0.81148, std: 0.02463, params: {'n_neighbors': 90},\n",
       " mean: 0.81405, std: 0.02362, params: {'n_neighbors': 91},\n",
       " mean: 0.81234, std: 0.03059, params: {'n_neighbors': 92},\n",
       " mean: 0.81063, std: 0.02799, params: {'n_neighbors': 93},\n",
       " mean: 0.81148, std: 0.02990, params: {'n_neighbors': 94},\n",
       " mean: 0.81320, std: 0.03030, params: {'n_neighbors': 95},\n",
       " mean: 0.81063, std: 0.02850, params: {'n_neighbors': 96},\n",
       " mean: 0.81063, std: 0.02627, params: {'n_neighbors': 97},\n",
       " mean: 0.81063, std: 0.02883, params: {'n_neighbors': 98},\n",
       " mean: 0.81063, std: 0.03025, params: {'n_neighbors': 99}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting ensemble of the best three classifiers from before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "best_classifiers = [\n",
    "    ('MultinomialNB', MultinomialNB(alpha=1.0)),\n",
    "    ('KNN', KNeighborsClassifier(6)),\n",
    "    ('SVM', SVC(gamma=2, C=1)),\n",
    "]\n",
    "\n",
    "voting_ensemble = sklearn.ensemble.VotingClassifier(best_classifiers)\n",
    "scores = cross_val_score(voting_ensemble, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.9486095170000001\n",
    "\n",
    "This is slightly worse than MultinomialNB from our previous experiment. We have run these experiments several times and sometimes MultinomialNB is worse (~93% accuracy), but in general it seems that MultimonialNB is about as good as or better than the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging ensemble of the best classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = MultinomialNB(alpha=1.0)\n",
    "bagging_ensemble = sklearn.ensemble.BaggingClassifier(base_classifier)\n",
    "scores = cross_val_score(bagging_ensemble, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.926422\n",
    "\n",
    "Again this is slightly worse than the standard MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting ensemble of the best classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = MultinomialNB(alpha=1.0)\n",
    "boosting_classifier = sklearn.ensemble.AdaBoostClassifier(base_classifier)\n",
    "scores = cross_val_score(boosting_classifier, train_input_vectors, train_labels, cv=10)\n",
    "print(\"Average 10-fold cross-validation accuracy: %f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Average 10-fold cross-validation accuracy: 0.666899\n",
    "\n",
    "This is significantly worse than standard MultinomialNB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "A voting ensemble, bagging ensemble, and boosting ensemble all achieved worse results than a standard MultinomialNB model. For this reason we will not use an ensemble for our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Now we turn our attention to exploring and analysing the data that we have cleaned, transformed, tokenized and saved into our data frame. We will look at a number of different features of the data, including:\n",
    "- Most-Common N-Grams\n",
    "- Most-Important N-Grams\n",
    "- Category Frequency and Imbalance\n",
    "- Distribution of Document Lengths\n",
    "- Category Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most-Common N-Grams\n",
    "\n",
    "We explore our corpus by looking at which words and phrases are most common. We break this down by document category in order to get a feel for which words are most indicative of a particular class, and therefore the words that should be considered most important by the classifier. \n",
    "\n",
    "We take the following steps:\n",
    "- We tokenise our data based on the n value for n-grams\n",
    "- We create a frequency distribution from the dataset for the n-grams\n",
    "- We convert this frequency distribution to a data frame which we can plot\n",
    "- We plot the data in the data frame\n",
    "\n",
    "To do this, we define a number of helper functions that modularise the code, which is good for reuse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dist_to_df(freq_dist, top_n):\n",
    "    \"\"\"\n",
    "    Convert a frequency distribution to a Data Frame, with columns 'term' and 'count'.\n",
    "    \"\"\"\n",
    "    most_common_terms = []\n",
    "    \n",
    "    for term, count in freq_dist.most_common(top_n):\n",
    "        most_common_terms.append({'term': term, 'count': count})\n",
    "    \n",
    "    most_common_df = pd.DataFrame(most_common_terms)\n",
    "    most_common_df.sort_values(by='count', ascending=True, inplace=True)\n",
    "    \n",
    "    # Change term labels from being (x, y, z) to \"x y z\".\n",
    "    join_tuple = lambda terms: \" \".join(terms)\n",
    "    most_common_df.term = most_common_df.term.apply(join_tuple)\n",
    "\n",
    "    return most_common_df\n",
    "\n",
    "\n",
    "def plot_frequency_df(df, title, xlabel=\"Frequency\", ylabel=\"\", fileName=None):\n",
    "    \"\"\"\n",
    "    Plot a horizontal bar chart of the most common words in a given \n",
    "    dataframe, with columns 'term' and 'count'. The bars are sorted \n",
    "    so that the most-common word appears at the top.\n",
    "    \"\"\"\n",
    "    fig, axis = plt.subplots()\n",
    "\n",
    "    df.plot.barh(x='term', y='count', ax=axis)\n",
    "    \n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel(xlabel)\n",
    "    axis.set_ylabel(ylabel)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure to an image file.\n",
    "    if fileName is not None:\n",
    "        plt.savefig(\"images/\" + fileName)\n",
    "\n",
    "    # This should only be executed after plt.savefig(),\n",
    "    # otherwise the image saved will be blank.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ngrams(ngrams_data, n=1, file_name=None):\n",
    "    \"\"\"\n",
    "    Plot the frequency of n-grams in a Series, for a given n.\n",
    "    \"\"\"\n",
    "    # Get all the tokens for the corpus in a list.\n",
    "    all_tokens_list = [token for tokens in ngrams_data for token in tokens]\n",
    "\n",
    "    ngrams = nltk.ngrams(all_tokens_list, n)\n",
    "\n",
    "    # Get the frequency of each term in the corpus and create a DataFrame from it.\n",
    "    ngram_frequency = nltk.FreqDist(ngrams)\n",
    "    ngrams_df = freq_dist_to_df(ngram_frequency, 5)\n",
    "    \n",
    "    ngram_name = \"Word\"  # default\n",
    "    if n == 2:\n",
    "        ngram_name = \"Bigram\"\n",
    "    elif n == 3:\n",
    "        ngram_name = \"Trigram\"\n",
    "\n",
    "    plot_frequency_df(ngrams_df, title=\"Top-10 Most-Common %s for Entire Corpus\" % ngram_name,\n",
    "                      xlabel=\"Frequency\", ylabel=ngram_name, fileName=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=1, file_name=\"most-common-unigram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=2, file_name=\"most-common-bigram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=3, file_name=\"most-common-trigram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Most Common Unigrams](images/most-common-unigram.png)\n",
    "\n",
    "The most-common words for the entire corpus don't really give us a huge amount of insight about the data. The most-common word is \"year\" but this could be used in any of the contexts of the document categories. Similarly for the word \"make\", \"people\" and \"time\". However, one word that does stand out is \"game\", which we would primarily associate with sport. However, it is possible that this word is applicable to other categories too. We will investigate the most-common words per category later to see if the game-sport assocation is supported.\n",
    "\n",
    "![Most Common Bigrams](images/most-common-bigram.png)\n",
    "![Most Common Trigrams](images/most-common-trigram.png)\n",
    "\n",
    "We get more information from analysing the bigrams and trigrams than the single words (also known as *unigrams*). For example, we see that the top bigram is \"tell bbc\" and the three most common trigrams are \"tell bbc news\", \"bbc news website\" and \"bbc news radio\". This would seem to suggest that the source of our news articles is BBC News. In fact, this is also supported by the reference to former-British Prime Minister, Tony Blair, and former-British Conservative Party Leader, Michael Howard, which are referenced by the fifth most common bigram and fourth most common trigram, respectively. \n",
    "\n",
    "Furthermore, we see that there are two bigrams that stand out for their association with particular document categories. \"Prime Minister\" is definitely associated with politics and \"Chief Executive\" is similarly associated with the business topic. We will investigate the importance of these n-grams later when dicussing the important terms and phrases for classifying each topic. \n",
    "\n",
    "The fifth most common trigram is \"million dollar baby\". This could be a reference to the boxing film of the same name that won multiple Oscars in 2005. This trigram could definitely be associated with the entertainment category. However, seeing as it's a film that is related to sports, it could also be associated in some way to the sports category. We will investigate the overlap between categories and their similarities later on in our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Frequency and Balance\n",
    "\n",
    "Let's look at the labels that we have been provided with for each of the documents in our corpus. There are five possible categories (or classes) that the label can represent: sport, entertainment, politics, technology and business. We also refer to our class labels as the *target label* of each document.\n",
    "\n",
    "We look at how many documents of each category appear in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df_handle.groupby(df_handle.category).category.count()\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise these absolute numbers to get a breakdown of the percentage frequency of each target class. This makes it easy to compare the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution_normalised = (class_distribution / sum(class_distribution)) * 100\n",
    "# Plot a line representing the mean frequency of all classes.\n",
    "# This is the frequency each class would have if uniformly distributed.\n",
    "mean_freq = 100 / len(class_distribution)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.axhline(y=mean_freq, color='orange')\n",
    "class_distribution_normalised.plot.bar(ax=axis, rot=0)\n",
    "axis.set_xlabel(\"Target Class (Document Category)\")\n",
    "axis.set_ylabel(\"Frequency (% of Corpus)\")\n",
    "axis.set_title(\"Target Class Frequencies Across Corpus\")\n",
    "\n",
    "plt.savefig('images/target-class-frequencies.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Target Class Frequencies Image](images/target-class-frequencies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a slighly unbalanced target class distribution. This means that not all target labels appear as often as each other. For example, the sport category is most common, whereas the entertainment category is the least common. When building our model, we need to take this into account, to make sure that we are not overly biasing our model based on the distribution of target classes in the training data. In extreme cases, our model could underfit the data and end up predicting the most common label (sport) each time. \n",
    "\n",
    "While this would be easy to spot and fix, it is possible that the class distribtion will affect the model in more subtle ways. We will investigate and evaluate this later, when we have made our predictions for the test dataset.\n",
    "\n",
    "## Distribution of Document Lengths\n",
    "\n",
    "Now, we look at some meta-statistics about each document. We compare the length of each document as well as the number of unique words in each document. We breakdown this analysis per-target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['document_length'] = df_handle.tokens.apply(len)\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "df_handle.plot.hist(by='document_length', bins=5, ax=axis[0], figsize=(10,4), legend=None)\n",
    "axis[0].set_xlabel(\"Document Length (Number of Tokens)\")\n",
    "axis[0].set_title(\"Document Length Distribution\")\n",
    "\n",
    "clipped_content_length_data = df_handle[(df_handle.document_length > 100) & (df_handle.document_length < 500)]\n",
    "clipped_content_length_data.plot.hist(by='document_length', bins=5, ax=axis[1], figsize=(10, 4), legend=None)\n",
    "axis[1].set_xlabel(\"Document Length (Number of Tokens)\")\n",
    "axis[1].set_title(\"Document Length Distribution (clipped)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/document-length-distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Document Length Distributions](images/document-length-distribution.png)\n",
    "\n",
    "From the first histogram on the left, we see that the overwhelming majority of documents have between 100 and 500 words. We don't get a huge amount of information from this plot so we replot the data, except that we filter the dataset so that we only include documents whose length is within this interval. This process is known as *clipping*. The clipped data is plotted on the right histogram.\n",
    "\n",
    "From the second histogram, we get more useful data. We see most of the documents have a length between 100 and 200 characters. It is important that we are aware that the distribution of the lengths of each document is not uniform. We also break this down per-target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length_per_class = df_handle.groupby(df_handle.category).document_length.mean()\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "\n",
    "doc_length_per_class.plot.bar(ax=axis, rot=0)\n",
    "\n",
    "# We plot the average document length for reference.\n",
    "mean_doc_length = df_handle['document_length'].mean()\n",
    "axis.axhline(y=mean_doc_length, color='orange')\n",
    "\n",
    "axis.set_xlabel(\"Target Class (Document Category)\")\n",
    "axis.set_ylabel(\"Mean Document Length (num. tokens)\")\n",
    "axis.set_title(\"Mean Document Length per Target Class\")\n",
    "\n",
    "plt.savefig(\"images/document-length-per-class.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mean Document Length per Target Class Bar Chart](images/document-length-per-class.png)\n",
    "\n",
    "Again, we see that we don't have uniform distribution of document length. This is important to recall, since, when we are using methods to vectorise the documents, if we don't take into account the documents' length, then we will be unfairly biasing longer or shorter documents (depending on the vectorising method). \n",
    "\n",
    "For example, if we are calculating the frequency of a term, comparing this across documents that have different lengths is unfair. A word that appears 10 times in a 50 word document is far more prominent that a word that appears 20 times in a 500 word document. So we need to make sure that we normalise our frequency calculation (i.e during TFIDF vectorisation) by the length of the doucment the term appears in. We will return to this point later when we are doing the document vectorisation.\n",
    "\n",
    "<!-- Anchor for internal links -->\n",
    "<a id='category_similarity'></a>\n",
    "\n",
    "## Category Similarity \n",
    "\n",
    "Now we turn our attention to computing which categories are most- and least-similar to each other. \n",
    "We do this with the following steps:\n",
    "- Take each of our TFIDF document vectors\n",
    "- Group the documents by their category\n",
    "- Compute the mean TFIDF vector for each category\n",
    "- Compute the cosine distance between each mean vector\n",
    "- Calculate the closest vectors and the vectors furthest apart\n",
    "\n",
    "We use cosine distance to as a measure of how similar each category is. Although distance measures how different each category is from each other, if we invert it then we get a similarity metric. This means that smaller distances imply greater similarity and larger distances mean less similarity. Cosine distance is a useful metric since it doesn't depend on the distance between the vectors in n-dimensional space, which could be unfairly biased by abnormally large vector-components. Instead, it depends on the angle between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE READ - Calculating TFIDF Vectors\n",
    "\n",
    "Note(mm): I have redone this here since it incorporates l2 normalisation and using ngrams in the TFIDFVectoriser. I have discussed this above in the data analysis. However, this could affect our models' results (particularly the KNNs since we are adding lots more features through the bigrams and trigram) and so we should evaluate this before including it in the main 'thread'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine each document's whole text content into a string.\n",
    "all_tokens = [token for token in df_handle.tokens]\n",
    "documents = [\" \".join(token) for token in all_tokens]\n",
    "\n",
    "# Normalize the TFIDF scores by the length of each document, including uni-, bi- and tri-grams.\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2', ngram_range=(1,3))\n",
    "\n",
    "# Perform the TFIDF calculations.\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_data = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                        columns=[tfidf_vectorizer.get_feature_names()],\n",
    "                        index=df_handle.index)\n",
    "\n",
    "tfidf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the mean TFIDF document vectors from the TFIDF matrix and put them into their own dataframe so that we can analyse them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = df_handle.category.unique()\n",
    "mean_vector_df = pd.DataFrame(index=tfidf_data.columns)\n",
    "\n",
    "for category in categories:\n",
    "    mean_vector = tfidf_data[df_handle.category == category].mean(axis=0)\n",
    "    mean_vector_df[category] = mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the distance between the categories, and not between the n-grams in the corpus, we need to transpose our data frame. This swaps the rows and the columns in the data frame. This means that the rows will now correspond to the categories and the columns to the n-grams.\n",
    "\n",
    "We have done this because it is more efficient to add to a dataframe column-wise. Adding categories' data column-by-column and then transposing is more efficient than if we did the transposition first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose our data frame.\n",
    "mean_vector_transpose = mean_vector_df.T\n",
    "\n",
    "# Calculate cosine distance based on the tranposed data.\n",
    "pairwise_cosine_distance = sklearn.metrics.pairwise_distances(mean_vector_transpose, metric='cosine')\n",
    "\n",
    "pairwise_cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could read this matrix by eye, but this could get complicated. Instead, we define some helper functions to find the most-similar categories and the least-similar categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_closest_categories(array, n):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of the category-pairs that are most similar to each other.\n",
    "    The number of tuples returned is decided by the size parameter, n.\n",
    "    \"\"\"\n",
    "    return n_extreme_categories(array, n, multiplier=1)\n",
    "\n",
    "\n",
    "def n_furthest_categories(array, n):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of the category-pairs that are furthest away from each other.\n",
    "    The number of tuples returned is decided by the size parameter, n.\n",
    "    \"\"\"\n",
    "    return n_extreme_categories(array, n, multiplier=-1)\n",
    "\n",
    "\n",
    "def n_extreme_categories(array, n, multiplier=None):\n",
    "    \"\"\"\n",
    "    Return a list of tuples of the category pairs that have the most-extreme distances from \n",
    "    each other. Extreme can mean either smallest or largest distance from each other. \n",
    "    \n",
    "    In the case of the calculating the largest, we sort our array in reverse. This means that\n",
    "    we first have to multiply the array by -1, since numpy only provide an ascending-order \n",
    "    sorter.\n",
    "   \n",
    "    Note that we want to use built-in numpy library functions since they are most-efficient in \n",
    "    dealing with numpy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We take the indices from 0 to 2n, with a step size of 2. This means that\n",
    "    # we skip every second index, which will be the same value as the index before it,\n",
    "    # given that the distance matrix is symmetric.\n",
    "    # For example, for the largest n=2, we take the first and third maximum elements, \n",
    "    # skipping the second since it will be the same as the first.\n",
    "    extreme_indices = np.argsort(multiplier * array.flatten())[0:2*n:2]\n",
    "\n",
    "    extreme_categories = []\n",
    "    \n",
    "    for ind in extreme_indices:\n",
    "        two_d_index = np.unravel_index(ind, array.shape)\n",
    "        \n",
    "        # Convert the tuple of indices to a tuple of the corresponding categories.\n",
    "        converted_tuple = tuple(map(lambda c: categories[c], two_d_index))\n",
    "        extreme_categories.append(converted_tuple)\n",
    "        \n",
    "    return extreme_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_furthest_categories(pairwise_cosine_distance, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the most-similar category-pairs we need to remove the zero values from our pairwise-distance array. The values are 0 when comparing a category to itself, but this doens't give us any valuable information. \n",
    "\n",
    "Instead, we will copy the pairwise-distance array and transform the zero-values to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_distances = pairwise_cosine_distance.copy()\n",
    "nonzero_distances[nonzero_distances == 0] = float(\"inf\")\n",
    "\n",
    "n_closest_categories(nonzero_distances, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the two most similar category-pairs are:\n",
    "- politics and buiness \n",
    "- business and tech\n",
    "\n",
    "And the two least similar category-pairs are:\n",
    "- sport and business\n",
    "- entertainment and business\n",
    "\n",
    "It is important the we recognise and compute these similarities when evaluating our model. It is a basic requirement that the model is able to distinguish between the least-similar categories of documents. In order to have an optimal model, it also needs to be able to tell apart the most-similar categories. \n",
    "\n",
    "However, when evaluating models, it is important to recognise that the category-pairs listed above are the most similar and, therefore, the hardest to tell apart. This means that we can still have a very good, high performance model, even if it is slightly confused between the most similar document categories.\n",
    "\n",
    "It is also interesting to note that the category of business is included in all of the most- and least-similar category-pairs. This indicates how complex and varied the defining-features of particular categories are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most-Important N-Grams for Each Category\n",
    "\n",
    "Now we look at which n-grams are the most-important for each category. We define importance by the TFIDF metric. By investigating the most important n-grams we should be able to determine the terms that are most indicative of a document's category.\n",
    "\n",
    "First of all, though, we look at the most important n-grams throughout the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean TFIDF vector for the entire corpus.\n",
    "tfidf_mean = tfidf_data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 10 largest TFIDF-valued terms. These can be uni-, bi-, or tri-grams.\n",
    "most_important_words = tfidf_mean.nlargest(10)\n",
    "\n",
    "## TODO(mm) worth comparing with the most common words?\n",
    "most_important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really get any valuable information from this list. However, it is interesting to note that all of the top ten n-grams are unigrams. This shows that bigrams and trigrams are much less indicative of a document's category than single words.\n",
    "\n",
    "Let's look at the most important bigram and the most important trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mean[tfidf_mean.index.map(lambda x: len(x.split())) == 2].nlargest(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mean[tfidf_mean.index.map(lambda x: len(x.split())) == 3].nlargest(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the most important bigram is an order of magnitude (10 times) less important than the top unigram. The most important trigram is an order of magnitude less important than that. \n",
    "\n",
    "This is interesting because it shows that, for n-grams longer than unigrams, the relative infrequency (TF)  (due to having to match multiple words in a pattern) is not outweighed by the inverse document frequency (IDF). The TFIDF values are lower as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we look at the most important words within each category, instead of across the entire corpus.\n",
    "Note that we have already calculated the mean TFIDF document vectors for each category in the [Category Similarity](#category_similarity) section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(len(categories), sharex=True)\n",
    "\n",
    "# Keep a list of all terms that appear in the top ten for each category.\n",
    "most_important_terms = []\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    category_vector = mean_vector_transpose.iloc[i]\n",
    "    \n",
    "    category_vector.nlargest(10).plot.barh(ax=axis[i], figsize=(10, 15))\n",
    "    \n",
    "    most_important_terms.extend(category_vector.nlargest(10).index)\n",
    "    \n",
    "    axis_title = \"Most Important N-Grams for Category %s\"\n",
    "    axis[i].set_title(axis_title % category_vector.name.title())\n",
    "    \n",
    "axis[len(categories) - 1].set_xlabel(\"Importance (TFIDF Value)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/most-important-terms-per-category.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Most Important Terms per Category](images/most-important-terms-per-category.png)\n",
    "\n",
    "These bar charts give us a wealth of insight into what is the defining words for each category. For example, by far the most important term is \"film\", which is indicative of the entertainment category. This means that a good model will identify the occurences of the word \"film\" in a document and make it more likely to classify that document as being about entertainment. \n",
    "\n",
    "We also get lots of information about the key words in each category. To start with, they are all unigrams (the lack of bigram or trigram phrases is discussed above). It is also interesting to note that the categories appear to be fairly disjoint from each other, based on their top ten most important words. We can see this by the fact that the only terms that is in the top ten in more than one category are:\n",
    "- \"game\", appearing for both the sport and tech categories\n",
    "- \"year\", appearing for both the business and entertainment categories\n",
    "\n",
    "We show this, computationally, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(most_important_terms).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when considering all terms, not just the top ten, we have seen that the categories are more aligned (see [Category Similarity](#category_similarity) above). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndPlotWordCloud(tokens):\n",
    "    # The word cloud uses a sentence string not a list,\n",
    "    # so we convert our tokens to a single string.\n",
    "    tokens_string = \" \".join(tokens)\n",
    "    \n",
    "    # Create the wordcloud object.\n",
    "    wordcloud = wc.WordCloud(width=1600, height=800).generate(tokens_string)\n",
    "    \n",
    "    # Matplotlib settings.\n",
    "    plt.figure(figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "\n",
    "def getAllTokens(df):\n",
    "    \"\"\"\n",
    "    Convert the tokens column in a dataframe into a list of strings of every \n",
    "    token in the tokens column.\n",
    "    \"\"\"\n",
    "    return [token for tokens in df['tokens'] for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the wordcloud image and show it. We also save it to a file called *corpus_wordcloud.png*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all the tokens from our data.\n",
    "all_tokens_list = getAllTokens(df_handle)\n",
    "\n",
    "wordcloud = createAndPlotWordCloud(all_tokens_list)\n",
    "wordcloud.to_file(\"images/corpus_wordcloud.png\")\n",
    "\n",
    "# Delete this since it's a variable that will be used later.\n",
    "del all_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO(mm): Analyse.\n",
    "\n",
    "![Wordcloud for Tokens of the Entire Corpus](images/corpus_wordcloud.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
