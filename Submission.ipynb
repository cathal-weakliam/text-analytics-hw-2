{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "- Preprocessing\n",
    "    - do data analysis on tokens\n",
    "\n",
    "- Test which preprocessing is best w/ simple model\n",
    "    - pick best by accuracy on task using simple model (Naive B)\n",
    "    - do data analysis on the vectors\n",
    "\n",
    "- Try out a bunch of different models; pick best three\n",
    "- Model 1 section\n",
    "    - discuss\n",
    "    - train\n",
    "    - hyperparameter optimization (using F1 averaged over classes)\n",
    "    - evaluate in detail\n",
    "- Model 2 section\n",
    "- Model 3 section\n",
    "\n",
    "- Try ensembling (evaluate using F1 again, decide if it's worth it)\n",
    "\n",
    "- Apply final model to test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Text Analytics - Homework 2\n",
    "### Group 1: Boluwade Alabi, Elizabeth Burke, Lilah Koudelka, Michael Mullen, Cathal Weakliam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1.0 - Preprocessing](#preprocess)\n",
    "     - [1.1 - Token Analysis](#token_analy)\n",
    "- [2.0 - Comparing Preprocessing Methods](#comp_methods)\n",
    "     - [2.1 - Highest Accuracy Produced Using a Simple Model](#best_model)\n",
    "     - [2.2 - Vector Data Analysis](#vect_analy)\n",
    "- [3.0 - Initial Model Building](#initial_models)\n",
    "     - [3.1 - Selecting the Top 3 Models](#top3)\n",
    "- [4.0 - Model #1](#m1)\n",
    "     - [4.1 - Discussion](#m1_discuss)\n",
    "     - [4.2 - Training](#m1_train)\n",
    "     - [4.3 - Hyperparameter Optimization](#m1_optimize)\n",
    "     - [4.4 - Evaluation](#m1_eval)\n",
    "- [5.0 - Model #2](#m2)\n",
    "     - [5.1 - Discussion](#m2_discuss)\n",
    "     - [5.2 - Training](#m2_train)\n",
    "     - [5.3 - Hyperparameter Optimization](#m2_optimize)\n",
    "     - [5.4 - Evaluation](#m2_eval)\n",
    "- [6.0 - Model #3](#m3)\n",
    "     - [6.1 - Discussion](#m3_discuss)\n",
    "     - [6.2 - Training](#m3_train)\n",
    "     - [6.3 - Hyperparameter Optimization](#m3_optimize)\n",
    "     - [6.4 - Evaluation](#m3_eval)\n",
    "- [7.0 - Ensembling](#ensemble)\n",
    "- [8.0 - Applying the Final Model to the Test Set](#apply_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.209302Z",
     "start_time": "2018-11-09T09:28:04.171309Z"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn\n",
    "import string\n",
    "import wordcloud as wc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data into Data Frame\n",
    "\n",
    "The very first thing we need to do before we can analyse our data is to gather the data from the input files and enter it into a Pandas DataFrame.\n",
    "\n",
    "### Read Documents Into a Data Frame\n",
    "\n",
    "We read the CSV files into a Pandas DataFrame object so we can easily analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "df_handle = pd.read_csv(\"trainingset.csv\", sep=\"^\", header=0)\n",
    "\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data Frame to CSV File\n",
    "\n",
    "Now that we have read the data into our dataframe, we will need a way to export the data. To do this, we write the dataframe to a CSV (comma-separated values) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.387334Z",
     "start_time": "2018-11-09T09:28:05.372305Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle.to_csv(\"h2_tokens.csv\", sep='^', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read our data from the file we just wrote to, in order to ensure that it was written to correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reread_data = pd.read_csv(\"h2_tokens.csv\", sep=\"^\")\n",
    "\n",
    "assert(reread_data.equals(df_handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully loaded our corpus into a data frame. \n",
    "Now we can do some cleaning and pre-processing steps on the data contained within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Tokens From Raw Text\n",
    "\n",
    "Now that we have our data, we want to process it. However, textual data is unstructured. This means that it can come in many different forms. We will have to develop a process by which to normalize the text so that the algorithms that we apply to each document will treat each document equally.\n",
    "\n",
    "\n",
    "To do this, we will split each document's textual content into a list of words, called tokens. This process is called *tokenization*. After tokenizing, we will perform some data cleaning and transformation (like removing special characters and converting every word to lowercase, etc.)\n",
    "\n",
    "We define each of our transformation functions individually. We also define a function, `normalize_text()`, that performs all of the transformation functions on a dataframe series.\n",
    "\n",
    "We have the individual functions so that we can go through and discuss them one-by-one in this section. In later sections, however, we will use the conglomerate function for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Functions\n",
    "\n",
    "Firstly, we define some utility functions that are needed by the transformation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.527333Z",
     "start_time": "2018-11-09T09:28:05.401304Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    \"\"\"\n",
    "    Returns a list of stopwords that should be removed when preprocessing text.\n",
    "    Included are nltk stopwords, salutations and a list of stopwords shown in the labs.\n",
    "    \"\"\"\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Add salutations to the stop words list.\n",
    "    salutations = ['mr','mrs','mss','dr','phd','prof','rev']\n",
    "    stop_words.extend(salutations)\n",
    "        \n",
    "    additional_stop_words = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "    stop_words.extend(additional_stop_words)\n",
    "    \n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def pos_to_wordnet_tag(pos_tag):\n",
    "    \"\"\"\n",
    "    Convert an NLTK.pos_tag to a WordNet tag that the lemmatizer uses.\n",
    "    \"\"\"\n",
    "    if pos_tag.lower().startswith('j'):\n",
    "        return 'a'\n",
    "    elif pos_tag.lower().startswith('v'):\n",
    "        return 'v'\n",
    "    elif pos_tag.lower().startswith('n'):\n",
    "        return 'n'\n",
    "    elif pos_tag.lower().startswith('r'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        # Default POS for lemmatization is noun.\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the transformation functions themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.622340Z",
     "start_time": "2018-11-09T09:28:05.528302Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_hyphens(series):\n",
    "    \"\"\"\n",
    "    Takes a string of text and returns a string of text with hyphens deleted.\n",
    "    We do this so words like \"e-mail\", \"wi-fi\", etc. are kept together.\n",
    "    \"\"\"\n",
    "    return series.replace(\"-\", \"\")\n",
    "\n",
    "\n",
    "def tokenize(series):\n",
    "    \"\"\"\n",
    "    Takes a string of text and returns a list of string tokens.\n",
    "    \"\"\"\n",
    "    # Pattern matches one or more alphanumeric characters (or underscores).\n",
    "    TOKENIZER_REGEX = r'\\w+'\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(TOKENIZER_REGEX)  \n",
    "    return tokenizer.tokenize(series)\n",
    "\n",
    "\n",
    "def decapitalize(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of tokens, with every token in lowercase form.\n",
    "    \"\"\"\n",
    "    # Map the string to-lowercase method to every value in the series.\n",
    "    return [word.lower() for word in series]\n",
    "\n",
    "\n",
    "def remove_numbers(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of tokens that do not consist solely of numeric characters.\n",
    "    For example, \"3\" is removed, but not \"3G\" or \"Three\".\n",
    "    \"\"\"\n",
    "    return [word for word in series if not word.isnumeric()]\n",
    "\n",
    "\n",
    "def remove_special_chars(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns that list, without special characters.\n",
    "    \n",
    "    The only special characters that are left in the text are underscores since we used\n",
    "    the regex '\\w+'.\n",
    "    \"\"\"    \n",
    "    return [word.replace(\"_\", \"\") for word in series]\n",
    "\n",
    "\n",
    "def remove_punctuation(series):\n",
    "    \"\"\" \n",
    "    Takes a list of tokens and returns a list of tokens, with any punctuation stripped.\n",
    "    \n",
    "    This is not included in the normalize_text() function since it is not needed - the\n",
    "    punctuation is removed in the tokenize() function anyway. However, it is included \n",
    "    here for completeness and is used in the n-gram analysis later.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for word in series:\n",
    "        new_word = \"\".join(character for character in word if character not in string.punctuation)\n",
    "        \n",
    "        if new_word is not \"\":\n",
    "            results.append(new_word)\n",
    "    \n",
    "    return results\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "def remove_stop_words(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of those tokens that are not contained \n",
    "    in the stop words list.\n",
    "    \"\"\"\n",
    "    return [word for word in series if word not in stop_words]\n",
    "    \n",
    "    \n",
    "def lemmatize(series):   \n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of the same tokens, transformed using the \n",
    "    WordNet Lemmatizer. This lemmatization process converts words into a common root.\n",
    "    For example, it converts plural words into singular form, or past-tense verbs into their root.\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    for word, tag in nltk.pos_tag(series):\n",
    "        wordnet_tag = pos_to_wordnet_tag(tag)\n",
    "        lemmatized_words.extend([lemmatizer.lemmatize(word, wordnet_tag)])\n",
    "        \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def normalize_text(series, keep_stop_words=False, lemmatization=True):\n",
    "    \"\"\" \n",
    "    Takes a pandas Series object and returns a list of tokens for that series.\n",
    "    Gives an option (default=True) on whether to do lemmatization or not.\n",
    "    \"\"\"\n",
    "    newseries = (series.apply(remove_hyphens)\n",
    "                  .apply(tokenize)\n",
    "                  .apply(remove_special_chars)\n",
    "                  .apply(decapitalize)\n",
    "                  .apply(remove_numbers))\n",
    "    \n",
    "    # Repeat removal of numbers after lemmatization. This is discussed below.\n",
    "    if lemmatization:\n",
    "        newseries = newseries.apply(lemmatize)\n",
    "        newseries = newseries.apply(remove_numbers)\n",
    "    \n",
    "    if not keep_stop_words:\n",
    "        newseries = newseries.apply(remove_stop_words)\n",
    "\n",
    "    return newseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go through applying each transformation function step-by-step.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Our first step is to tokenize the words in the text of each article. We will create a new column in our dataframe and call it `tokens`, filling it with the result of applying our `tokenize()` function on the `contents` column.\n",
    "\n",
    "We define a token using the regular expression `\\w+`, which matches any alphanumeric character or an underscore, \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.647329Z",
     "start_time": "2018-11-09T09:28:05.624336Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['content'].apply(remove_hyphens).apply(tokenize)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have successsfully split the `content` column into its component words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation and Special Characters\n",
    "\n",
    "Because of the regex pattern that we used, `\\w+`, we have already separated tokens into only alphanumeric words. This means that we don't have to do any further filtering to remove punctuation and special characters. We also remove any tokens that are just empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.674335Z",
     "start_time": "2018-11-09T09:28:05.657303Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_special_chars)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decapitalization\n",
    "\n",
    "Our data is still not fully normalized though. We want all tokens to have the same form, but some of the tokens are uppercase, some lowercase, and some a mixture of both. To ensure consistency in our data, we are going to convert every token into its lowercase form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.704304Z",
     "start_time": "2018-11-09T09:28:05.683303Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(decapitalize)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Numbers\n",
    "\n",
    "We can see that some of our tokens are just numeric strings, which we want to remove.\n",
    "\n",
    "Note that we will not remove tokens that contain both letters and numbers. For example, \"3g\" considered a word and it would lose its meaning if it were changed to just \"g\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.718308Z",
     "start_time": "2018-11-09T09:28:05.705303Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_numbers)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Consider the second document. We have tokens like \"games\" and \"pixels\" - they are plural nouns. However, if we want to count the occurrences of these tokens, they should not be considered as different words to their singular counterparts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.727334Z",
     "start_time": "2018-11-09T09:28:05.719306Z"
    }
   },
   "outputs": [],
   "source": [
    "first_article = df_handle.tokens.iloc[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.738303Z",
     "start_time": "2018-11-09T09:28:05.728304Z"
    }
   },
   "outputs": [],
   "source": [
    "count_games = first_article.count(\"games\")\n",
    "count_game = first_article.count(\"game\")\n",
    "\n",
    "print(\"Count of 'games' = {}\".format(count_games))\n",
    "print(\"Count of 'game'  = {}\".format(count_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform an operation known as *lemmatization*, which takes a word and alters it to its root form. For example, it will convert plural nouns to singular nouns, or convert past-tense verbs to the radical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:07.927304Z",
     "start_time": "2018-11-09T09:28:05.739307Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(lemmatize)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the lemmatizer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(df_handle.tokens.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look to see if this has amalgamated the words 'game' and 'games':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:07.933304Z",
     "start_time": "2018-11-09T09:28:07.928306Z"
    }
   },
   "outputs": [],
   "source": [
    "count_game = df_handle['tokens'].iloc[1].count(\"game\")\n",
    "print(\"Count of 'game'  = {}\".format(count_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has, so our lemmatization has been successful.\n",
    "\n",
    "However, we notice that numbers have reappeared in our token set. This is because our remove_numbers() function didn't remove a word like \"20s\" because it wasn't all numeric, however, it was lemmatized back to \"20\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['tokens'].iloc[1].count(\"20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize(['20s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat this, we are going to rerun our remove_numbers function after having applied lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_numbers)\n",
    "\n",
    "df_handle['tokens'].iloc[1].count(\"20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop Words\n",
    "\n",
    "We have now cleaned up our tokens so that they only contain real words and in lemmatized form. However, some words are not as valuable as others. For example, if we want to calculate the most-common word in an article, it is likely that it will be a word like 'a', 'the', etc. These words are called *stop words*, they do not gives us valuable information since they are so prevalent in the English language. So we decide that we are going to remove them from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:12.750003Z",
     "start_time": "2018-11-09T09:28:07.934304Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_stop_words)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "#### Test Cleaned Data\n",
    "\n",
    "It is very important that we run tests to ensure our data is as we expect it to be. Failing to carry out tests can lead to problems in analysis further on.\n",
    "\n",
    "We define and run a test now to ensure that our data has been tokenized successfully and those tokens are cleaned and normalized as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:12.768564Z",
     "start_time": "2018-11-09T09:28:12.751140Z"
    }
   },
   "outputs": [],
   "source": [
    "def columnIsClean(series):\n",
    "    return all([tokensAreClean(tokens) for tokens in series])\n",
    "   \n",
    "\n",
    "def tokensAreClean(tokens):    \n",
    "    if any([token.isupper() for token in tokens]):\n",
    "        print(\"Some tokens are not uppercase\")\n",
    "        return False\n",
    "    \n",
    "    elif any([not token.isalnum() for token in tokens]):\n",
    "        print(\"Some tokens contain special (non-alphanumeric) characters\")\n",
    "        return False\n",
    "    \n",
    "    elif any([token.isnumeric() for token in tokens]):\n",
    "        print(\"Some tokens are numeric\")\n",
    "        return False\n",
    "    \n",
    "    elif any([True for token in tokens if token in get_stop_words()]):\n",
    "        print(\"Some stop words were not removed\")\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        # Data has been cleaned successfully.\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:15.022271Z",
     "start_time": "2018-11-09T09:28:12.770564Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cleaned_successfully = columnIsClean(df_handle['tokens'])\n",
    "\n",
    "# Throw an error if test fails.\n",
    "assert(data_cleaned_successfully)\n",
    "\n",
    "if data_cleaned_successfully:\n",
    "    print(\"Data Cleaned and Tokenized Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do Tokenization in One Go\n",
    "\n",
    "Now that we have shown that our data has been successfully cleaned, we also need to test that our function to do the normalization in one go (which we will use later) is equivalent to what we have done step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:20.549167Z",
     "start_time": "2018-11-09T09:28:15.023267Z"
    }
   },
   "outputs": [],
   "source": [
    "oldtokens = df_handle['tokens'].copy()\n",
    "\n",
    "df_handle['tokens'] = normalize_text(df_handle['content'])\n",
    "\n",
    "series_equal = oldtokens.equals(df_handle['tokens'])\n",
    "\n",
    "# Throw an error if not equal.\n",
    "assert(series_equal)\n",
    "\n",
    "if series_equal:\n",
    "    print(\"The one-by-one and all-at-once methods are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='token_analy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Token Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comp_methods'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Comparing Preprocessing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Highest Accuracy Produced Using a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vect_analy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Vector Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initial_models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 - Initial Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Selecting the Top 3 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 - Model #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_discuss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_optimize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 - Model #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_discuss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_optimize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 - Model #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_discuss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_optimize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ensemble'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 - Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='apply_final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 - Applying the Final Model to the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
