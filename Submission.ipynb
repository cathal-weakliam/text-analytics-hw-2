{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Text Analytics - Homework 2\n",
    "\n",
    "Group 1:\n",
    "- Boluwade Alabi\n",
    "- Elizabeth Burke\n",
    "- Lilah Koudelka\n",
    "- Michael Mullen\n",
    "- Cathal Weakliam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Gather-Data-into-Data-Frame\" data-toc-modified-id=\"Gather-Data-into-Data-Frame-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Gather Data into Data Frame</a></span></li><li><span><a href=\"#Extract-Tokens-From-Raw-Text\" data-toc-modified-id=\"Extract-Tokens-From-Raw-Text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Extract Tokens From Raw Text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transformation-Functions\" data-toc-modified-id=\"Transformation-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Transformation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Remove-Punctuation-and-Special-Characters\" data-toc-modified-id=\"Remove-Punctuation-and-Special-Characters-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Remove Punctuation and Special Characters</a></span></li><li><span><a href=\"#Decapitalization\" data-toc-modified-id=\"Decapitalization-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Decapitalization</a></span></li><li><span><a href=\"#Remove-Numbers\" data-toc-modified-id=\"Remove-Numbers-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Remove Numbers</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Lemmatization</a></span></li><li><span><a href=\"#Remove-Stop-Words\" data-toc-modified-id=\"Remove-Stop-Words-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>Remove Stop Words</a></span></li></ul></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-Cleaned-Data\" data-toc-modified-id=\"Test-Cleaned-Data-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Test Cleaned Data</a></span></li><li><span><a href=\"#Do-Tokenization-in-One-Go\" data-toc-modified-id=\"Do-Tokenization-in-One-Go-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Do Tokenization in One Go</a></span></li></ul></li><li><span><a href=\"#Save-Data-Frame-to-CSV-File\" data-toc-modified-id=\"Save-Data-Frame-to-CSV-File-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Save Data Frame to CSV File</a></span></li></ul></li><li><span><a href=\"#Token-Analysis\" data-toc-modified-id=\"Token-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Token Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Most-Common-N-Grams\" data-toc-modified-id=\"Most-Common-N-Grams-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Most-Common N-Grams</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Cloud\" data-toc-modified-id=\"Word-Cloud-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Word Cloud</a></span></li><li><span><a href=\"#Bar-Charts\" data-toc-modified-id=\"Bar-Charts-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Bar Charts</a></span></li></ul></li><li><span><a href=\"#Category-Frequency-and-Balance\" data-toc-modified-id=\"Category-Frequency-and-Balance-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Category Frequency and Balance</a></span></li><li><span><a href=\"#Distribution-of-Document-Lengths\" data-toc-modified-id=\"Distribution-of-Document-Lengths-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Distribution of Document Lengths</a></span></li></ul></li><li><span><a href=\"#TFIDF-Vectorisation\" data-toc-modified-id=\"TFIDF-Vectorisation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TFIDF Vectorisation</a></span></li><li><span><a href=\"#TFIDF-Vector-Analysis\" data-toc-modified-id=\"TFIDF-Vector-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TFIDF Vector Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Category-Similarity\" data-toc-modified-id=\"Category-Similarity-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Category Similarity</a></span></li><li><span><a href=\"#Most-Important-Words\" data-toc-modified-id=\"Most-Important-Words-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Most-Important Words</a></span><ul class=\"toc-item\"><li><span><a href=\"#Across-Entire-Corpus\" data-toc-modified-id=\"Across-Entire-Corpus-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Across Entire Corpus</a></span></li><li><span><a href=\"#Within-Each-Category\" data-toc-modified-id=\"Within-Each-Category-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Within Each Category</a></span></li></ul></li></ul></li><li><span><a href=\"#Initial-Model-Building\" data-toc-modified-id=\"Initial-Model-Building-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Initial Model Building</a></span></li><li><span><a href=\"#Model-#1-KNN\" data-toc-modified-id=\"Model-#1-KNN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model #1 KNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href=\"#Hyperparameter-Optimization\" data-toc-modified-id=\"Hyperparameter-Optimization-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Hyperparameter Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Results:\" data-toc-modified-id=\"Results:-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Results:</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Model-#2:-Naive-Bayes\" data-toc-modified-id=\"Model-#2:-Naive-Bayes-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model #2: Naive Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href=\"#Hyperparameter-Optimization\" data-toc-modified-id=\"Hyperparameter-Optimization-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Hyperparameter Optimization</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Model-#3-Logistic-Regression\" data-toc-modified-id=\"Model-#3-Logistic-Regression-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Model #3 Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Hyperparameter-Optimization\" data-toc-modified-id=\"Hyperparameter-Optimization-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Hyperparameter Optimization</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Ensembling\" data-toc-modified-id=\"Ensembling-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Ensembling</a></span></li><li><span><a href=\"#Applying-the-Final-Model-to-the-Test-Set\" data-toc-modified-id=\"Applying-the-Final-Model-to-the-Test-Set-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Applying the Final Model to the Test Set</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan\n",
    "- Preprocessing\n",
    "    - do data analysis on tokens\n",
    "\n",
    "- Test which preprocessing is best w/ simple model\n",
    "    - pick best by accuracy on task using simple model (Naive B)\n",
    "    - do data analysis on the vectors\n",
    "\n",
    "- Try out a bunch of different models; pick best three\n",
    "- Model 1 section\n",
    "    - discuss\n",
    "    - train\n",
    "    - hyperparameter optimization (using F1 averaged over classes)\n",
    "    - evaluate in detail\n",
    "- Model 2 section\n",
    "- Model 3 section\n",
    "\n",
    "- Try ensembling (evaluate using F1 again, decide if it's worth it)\n",
    "\n",
    "- Apply final model to test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all of the required modules for running our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.209302Z",
     "start_time": "2018-11-09T09:28:04.171309Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import wordcloud as wc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data into Data Frame\n",
    "\n",
    "The very first thing we need to do before we can analyse our data is to gather the data from the input files and enter it into a Pandas DataFrame.\n",
    "\n",
    "We read the CSV files into a Pandas DataFrame object so we can easily analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle = pd.read_csv(\"trainingset.csv\", sep=\"^\", header=0)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully loaded our corpus into a data frame. \n",
    "Now we can do some cleaning and pre-processing steps on the data contained within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Tokens From Raw Text\n",
    "\n",
    "Now that we have our data, we want to process it. However, textual data is unstructured. This means that it can come in many different forms. We will have to develop a process by which to normalize the text so that the algorithms that we apply to each document will treat each document equally.\n",
    "\n",
    "\n",
    "To do this, we will split each document's textual content into a list of words, called tokens. This process is called *tokenization*. After tokenizing, we will perform some data cleaning and transformation (like removing special characters and converting every word to lowercase, etc.)\n",
    "\n",
    "We define each of our transformation functions individually. We also define a function, `normalize_text()`, that performs all of the transformation functions on a dataframe series.\n",
    "\n",
    "We have the individual functions so that we can go through and discuss them one-by-one in this section. In later sections, however, we will use the conglomerate function for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Functions\n",
    "\n",
    "Firstly, we define some utility functions that are needed by the transformation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.527333Z",
     "start_time": "2018-11-09T09:28:05.401304Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    \"\"\"\n",
    "    Returns a list of stopwords that should be removed when preprocessing text.\n",
    "    Included are nltk stopwords, salutations and a list of stopwords shown in the labs.\n",
    "    \"\"\"\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Add salutations to the stop words list.\n",
    "    salutations = ['mr','mrs','mss','dr','phd','prof','rev']\n",
    "    stop_words.extend(salutations)\n",
    "\n",
    "    additional_stop_words = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "    stop_words.extend(additional_stop_words)\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def pos_to_wordnet_tag(pos_tag):\n",
    "    \"\"\"\n",
    "    Convert an NLTK.pos_tag to a WordNet tag that the lemmatizer uses.\n",
    "    \"\"\"\n",
    "    if pos_tag.lower().startswith('j'):\n",
    "        return 'a'\n",
    "    elif pos_tag.lower().startswith('v'):\n",
    "        return 'v'\n",
    "    elif pos_tag.lower().startswith('n'):\n",
    "        return 'n'\n",
    "    elif pos_tag.lower().startswith('r'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        # Default POS for lemmatization is noun.\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the transformation functions themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.622340Z",
     "start_time": "2018-11-09T09:28:05.528302Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_hyphens(series):\n",
    "    \"\"\"\n",
    "    Takes a string of text and returns a string of text with hyphens deleted.\n",
    "    We do this so words like \"e-mail\", \"wi-fi\", etc. are kept together.\n",
    "    \"\"\"\n",
    "    return series.replace(\"-\", \"\")\n",
    "\n",
    "\n",
    "def tokenize(series):\n",
    "    \"\"\"\n",
    "    Takes a string of text and returns a list of string tokens.\n",
    "    \"\"\"\n",
    "    # Pattern matches one or more alphanumeric characters (or underscores).\n",
    "    TOKENIZER_REGEX = r'\\w+'\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(TOKENIZER_REGEX)  \n",
    "    return tokenizer.tokenize(series)\n",
    "\n",
    "\n",
    "def decapitalize(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of tokens, with every token in lowercase form.\n",
    "    \"\"\"\n",
    "    # Map the string to-lowercase method to every value in the series.\n",
    "    return [word.lower() for word in series]\n",
    "\n",
    "\n",
    "def remove_numbers(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of tokens that do not consist solely of numeric characters.\n",
    "    For example, \"3\" is removed, but not \"3G\" or \"Three\".\n",
    "    \"\"\"\n",
    "    return [word for word in series if not word.isnumeric()]\n",
    "\n",
    "\n",
    "def remove_special_chars(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns that list, without special characters.\n",
    "    \n",
    "    The only special characters that are left in the text are underscores since we used\n",
    "    the regex '\\w+'.\n",
    "    \"\"\"    \n",
    "    return [word.replace(\"_\", \"\") for word in series]\n",
    "\n",
    "\n",
    "def remove_punctuation(series):\n",
    "    \"\"\" \n",
    "    Takes a list of tokens and returns a list of tokens, with any punctuation stripped.\n",
    "    \n",
    "    This is not included in the normalize_text() function since it is not needed - the\n",
    "    punctuation is removed in the tokenize() function anyway. However, it is included \n",
    "    here for completeness and is used in the n-gram analysis later.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for word in series:\n",
    "        new_word = \"\".join(character for character in word if character not in string.punctuation)\n",
    "        \n",
    "        if new_word is not \"\":\n",
    "            results.append(new_word)\n",
    "    \n",
    "    return results\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "def remove_stop_words(series):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of those tokens that are not contained \n",
    "    in the stop words list.\n",
    "    \"\"\"\n",
    "    return [word for word in series if word not in stop_words]\n",
    "    \n",
    "    \n",
    "def lemmatize(series):   \n",
    "    \"\"\"\n",
    "    Takes a list of tokens and returns a list of the same tokens, transformed using the \n",
    "    WordNet Lemmatizer. This lemmatization process converts words into a common root.\n",
    "    For example, it converts plural words into singular form, or past-tense verbs into their root.\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    for word, tag in nltk.pos_tag(series):\n",
    "        wordnet_tag = pos_to_wordnet_tag(tag)\n",
    "        lemmatized_words.extend([lemmatizer.lemmatize(word, wordnet_tag)])\n",
    "        \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def normalize_text(series, keep_stop_words=False, lemmatization=True):\n",
    "    \"\"\" \n",
    "    Takes a pandas Series object and returns a list of tokens for that series.\n",
    "    Gives an option (default=True) on whether to do lemmatization or not.\n",
    "    \"\"\"\n",
    "    newseries = (series.apply(remove_hyphens)\n",
    "                  .apply(tokenize)\n",
    "                  .apply(remove_special_chars)\n",
    "                  .apply(decapitalize)\n",
    "                  .apply(remove_numbers))\n",
    "    \n",
    "    # Repeat removal of numbers after lemmatization. This is discussed below.\n",
    "    if lemmatization:\n",
    "        newseries = newseries.apply(lemmatize)\n",
    "        newseries = newseries.apply(remove_numbers)\n",
    "    \n",
    "    if not keep_stop_words:\n",
    "        newseries = newseries.apply(remove_stop_words)\n",
    "\n",
    "    return newseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go through applying each transformation function step-by-step.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Our first step is to tokenize the words in the text of each article. We will create a new column in our dataframe and call it `tokens`, filling it with the result of applying our `tokenize()` function on the `contents` column.\n",
    "\n",
    "We define a token using the regular expression `\\w+`, which matches any alphanumeric character or an underscore, \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.647329Z",
     "start_time": "2018-11-09T09:28:05.624336Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['content'].apply(remove_hyphens).apply(tokenize)\n",
    "df_handle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have successsfully split the `content` column into its component words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation and Special Characters\n",
    "\n",
    "Because of the regex pattern that we used, `\\w+`, we have already separated tokens into only alphanumeric words. This means that we don't have to do any further filtering to remove punctuation and special characters. We also remove any tokens that are just empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.674335Z",
     "start_time": "2018-11-09T09:28:05.657303Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_special_chars)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decapitalization\n",
    "\n",
    "Our data is still not fully normalized though. We want all tokens to have the same form, but some of the tokens are uppercase, some lowercase, and some a mixture of both. To ensure consistency in our data, we are going to convert every token into its lowercase form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.704304Z",
     "start_time": "2018-11-09T09:28:05.683303Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(decapitalize)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Numbers\n",
    "\n",
    "We can see that some of our tokens are just numeric strings, which we want to remove.\n",
    "\n",
    "Note that we will not remove tokens that contain both letters and numbers. For example, \"3g\" considered a word and it would lose its meaning if it were changed to just \"g\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.718308Z",
     "start_time": "2018-11-09T09:28:05.705303Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_numbers)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Consider the second document. We have tokens like \"games\" and \"pixels\" - they are plural nouns. However, if we want to count the occurrences of these tokens, they should not be considered as different words to their singular counterparts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.727334Z",
     "start_time": "2018-11-09T09:28:05.719306Z"
    }
   },
   "outputs": [],
   "source": [
    "first_article = df_handle.tokens.iloc[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.738303Z",
     "start_time": "2018-11-09T09:28:05.728304Z"
    }
   },
   "outputs": [],
   "source": [
    "count_games = first_article.count(\"games\")\n",
    "count_game = first_article.count(\"game\")\n",
    "\n",
    "print(\"Count of 'games' = {}\".format(count_games))\n",
    "print(\"Count of 'game'  = {}\".format(count_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform an operation known as *lemmatization*, which takes a word and alters it to its root form. For example, it will convert plural nouns to singular nouns, or convert past-tense verbs to the radical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:07.927304Z",
     "start_time": "2018-11-09T09:28:05.739307Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(lemmatize)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the lemmatizer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(df_handle.tokens.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look to see if this has amalgamated the words 'game' and 'games':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:07.933304Z",
     "start_time": "2018-11-09T09:28:07.928306Z"
    }
   },
   "outputs": [],
   "source": [
    "count_game = df_handle['tokens'].iloc[1].count(\"game\")\n",
    "print(\"Count of 'game'  = {}\".format(count_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has, so our lemmatization has been successful.\n",
    "\n",
    "However, we notice that numbers have reappeared in our token set. This is because our `remove_numbers()` function didn't remove a word like \"20s\" because it wasn't all numeric, however, it was lemmatized back to \"20\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['tokens'].iloc[1].count(\"20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize(['20s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat this, we are going to rerun our `remove_numbers()` function after having applied lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_numbers)\n",
    "df_handle['tokens'].iloc[1].count(\"20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop Words\n",
    "\n",
    "We have now cleaned up our tokens so that they only contain real words and in lemmatized form. However, some words are not as valuable as others. For example, if we want to calculate the most-common word in an article, it is likely that it will be a word like 'a', 'the', etc. These words are called *stop words*. They do not gives us valuable information since they are so prevalent in the English language. So we decided that we are going to remove them from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:12.750003Z",
     "start_time": "2018-11-09T09:28:07.934304Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle['tokens'] = df_handle['tokens'].apply(remove_stop_words)\n",
    "df_handle['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "#### Test Cleaned Data\n",
    "\n",
    "It is very important that we run tests to ensure our data is as we expect it to be. Failing to carry out tests can lead to problems in analysis further on.\n",
    "\n",
    "We define and run a test now to ensure that our data has been tokenized successfully and those tokens are cleaned and normalized as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:12.768564Z",
     "start_time": "2018-11-09T09:28:12.751140Z"
    }
   },
   "outputs": [],
   "source": [
    "def columnIsClean(series):\n",
    "    return all([tokensAreClean(tokens) for tokens in series])\n",
    "   \n",
    "\n",
    "def tokensAreClean(tokens):    \n",
    "    if any([token.isupper() for token in tokens]):\n",
    "        print(\"Some tokens are not uppercase\")\n",
    "        return False\n",
    "    \n",
    "    elif any([not token.isalnum() for token in tokens]):\n",
    "        print(\"Some tokens contain special (non-alphanumeric) characters\")\n",
    "        return False\n",
    "    \n",
    "    elif any([token.isnumeric() for token in tokens]):\n",
    "        print(\"Some tokens are numeric\")\n",
    "        return False\n",
    "    \n",
    "    elif any([True for token in tokens if token in get_stop_words()]):\n",
    "        print(\"Some stop words were not removed\")\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        # Data has been cleaned successfully.\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:15.022271Z",
     "start_time": "2018-11-09T09:28:12.770564Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cleaned_successfully = columnIsClean(df_handle['tokens'])\n",
    "\n",
    "# Throw an error if test fails.\n",
    "assert(data_cleaned_successfully)\n",
    "\n",
    "if data_cleaned_successfully:\n",
    "    print(\"Data Cleaned and Tokenized Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do Tokenization in One Go\n",
    "\n",
    "Now that we have shown that our data has been successfully cleaned, we also need to test that our function to do the normalization in one go (which we will use later) is equivalent to what we have done step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:20.549167Z",
     "start_time": "2018-11-09T09:28:15.023267Z"
    }
   },
   "outputs": [],
   "source": [
    "oldtokens = df_handle['tokens'].copy()\n",
    "\n",
    "df_handle['tokens'] = normalize_text(df_handle['content'])\n",
    "\n",
    "series_equal = oldtokens.equals(df_handle['tokens'])\n",
    "\n",
    "# Throw an error if not equal.\n",
    "assert(series_equal)\n",
    "\n",
    "if series_equal:\n",
    "    print(\"The one-by-one and all-at-once methods are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data Frame to CSV File\n",
    "\n",
    "Now that we have read and processed our data, we export it to a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T09:28:05.387334Z",
     "start_time": "2018-11-09T09:28:05.372305Z"
    }
   },
   "outputs": [],
   "source": [
    "df_handle.to_csv(\"h2_tokens.csv\", sep='^', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that our dataframe was written to a CSV file correctly, we will re-read it again. However, because we had lists of strings in our dataframe, we will need to do some parsing to ensure that they are read back into a dataframe in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertStringList(stringList):\n",
    "    \"\"\"\n",
    "    Takes a string representation of a list of strings, returns the actual list of strings object.\n",
    "    e.g. \"['one', 'two']\" -> [\"one\", \"two\"]\n",
    "    \"\"\"\n",
    "    # Removes all of the apostrophes and brackets.\n",
    "    regex = r'\\w+'\n",
    "    regex_tokenizer = nltk.RegexpTokenizer(regex)\n",
    "    \n",
    "    # Create and return a list of strings from the string representation of a list of strings.\n",
    "    return regex_tokenizer.tokenize(stringList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reread_data = pd.read_csv(\"h2_tokens.csv\", sep=\"^\")\n",
    "reread_data['tokens'] = reread_data['tokens'].apply(convertStringList)\n",
    "\n",
    "are_equal = reread_data.equals(df_handle)\n",
    "\n",
    "if are_equal:\n",
    "    print(\"The two data frames are equal!\")\n",
    "else:\n",
    "    # Throw an exception.\n",
    "    raise AssertionError(\"The two data frames are not the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='token_analy'></a>\n",
    "## Token Analysis\n",
    "Now we turn our attention to exploring and analysing the data that we have cleaned, transformed, tokenized and saved into our data frame. In particular, we will investigate the tokens columns that we have constructed. We will look at the following features:\n",
    "- Most-Common N-Grams\n",
    "- Category Frequency and Imbalance\n",
    "- Distribution of Document Lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most-Common N-Grams\n",
    "We start our analysis by look at the most common words and phrases in the corpus. This will give us a better feel for the data that we are dealing with and may give us some useful insights into  particular words or phrases our classifier should pay most attention to.\n",
    "\n",
    "We will also analyse the best ways to visualise our results, comparing word clouds with simple bar charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud\n",
    "We explore our corpus by looking at which words and phrases are most common. One popular way of doing this is by creating a word cloud from the corpus data.\n",
    "\n",
    "We create the wordcloud image using the `wordcloud` library and plot it. \n",
    "<br />\n",
    "We are also saving it to a file called *corpus_wordcloud.png*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tokens from our data.\n",
    "all_tokens_list = [token for tokens in df_handle['tokens'] for token in tokens]\n",
    "\n",
    "# The word cloud uses a single string not a list, so we convert our \n",
    "# list of tokens to a single string.\n",
    "all_tokens_string = \" \".join(all_tokens_list)\n",
    "\n",
    "# Create the WordCloud object from the string of tokens.\n",
    "wordcloud = wc.WordCloud(width=1600, height=800).generate(all_tokens_string)\n",
    "\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "\n",
    "# Save the word cloud image to a file.\n",
    "wordcloud.to_file(\"images/corpus_wordcloud.png\")\n",
    "\n",
    "# Delete this to clean up our notebook.\n",
    "del all_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the word cloud, a number of words stand out above the rest, for example \"year\", \"make\" and \"people\". Due to their bigger size, we can conclude that they are the most frequent words in our corpus. However, it is hard to draw many more conclusions from the word cloud. There are a number of problems associated with it:\n",
    "- The length of words in the word cloud biases our perception of it. For example, longer words like \"government\" stand out more than other smaller words that have roughly the same size, e.g. \"uk\".\n",
    "- Furthermore, it is quite hard to judge the font size of each word. Obviously, when there is a big difference we can distinguish it, but it is hard to make judgements in subtler cases. This is particularly true when words are plotted far apart in the word cloud. For instance, which word is more common, \"market\" or \"world\"?\n",
    "- All of the words that are plotted are unigrams. Word clouds don't take bigrams, trigrams, etc. into account, so we lose information we could potentially gain from exploring them. For example, \"labour\" and \"party\" appear to have similar frequencies, but perhaps they are part of the bigram referring to the Labour Party, a political organisation? We have no way of telling from this plot.\n",
    "- Finally, the colour of the words in the word cloud is confusing. While it makes the word cloud more appealing to look at, there is no information encoded by the colour. The perception we have of words is also biased based on which colours we prefer or the colours that stand out most to us. \n",
    "\n",
    "For these reasons, we don't believe word clouds are a good way of analysing our dataset. Instead, we will focus on simple bar charts, that make it easier to tell the frequency of a particular term and to compare between different terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Charts\n",
    "Having shown that word clouds are not the best method for visualising our data, we move on to creating bar charts. To create a bar chart, we need to get the frequency data for unigrams, bigrams, trigrams, etc. We do this by using the `FreqDist` object from the `nltk` library.\n",
    "\n",
    "The steps we follow are:\n",
    "- Extract our n-grams (for a given n) from our list of tokens\n",
    "- We create a frequency distribution from the dataset for the n-grams\n",
    "- We convert this frequency distribution to a data frame which we can plot\n",
    "- We plot the data in the data frame\n",
    "\n",
    "To do this, we define a number of helper functions that modularise the code, which is good for reuse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dist_to_df(freq_dist, top_n):\n",
    "    \"\"\"\n",
    "    Convert a frequency distribution to a Data Frame, with columns 'term' and 'count'.\n",
    "    \"\"\"\n",
    "    most_common_terms = []\n",
    "    \n",
    "    for term, count in freq_dist.most_common(top_n):\n",
    "        most_common_terms.append({'term': term, 'count': count})\n",
    "    \n",
    "    most_common_df = pd.DataFrame(most_common_terms)\n",
    "    most_common_df.sort_values(by='count', ascending=True, inplace=True)\n",
    "    \n",
    "    # Change term labels from being \"(x, y, z)\" to \"x y z\".\n",
    "    join_tuple = lambda terms: \" \".join(terms)\n",
    "    most_common_df.term = most_common_df.term.apply(join_tuple)\n",
    "\n",
    "    return most_common_df\n",
    "\n",
    "\n",
    "def plot_frequency_df(df, title, xlabel=\"Frequency\", ylabel=\"\", fileName=None):\n",
    "    \"\"\"\n",
    "    Plot a horizontal bar chart of the most common words in a given \n",
    "    dataframe, with columns 'term' and 'count'. The bars are sorted \n",
    "    so that the most-common word appears at the top.\n",
    "    \"\"\"\n",
    "    fig, axis = plt.subplots()\n",
    "\n",
    "    df.plot.barh(x='term', y='count', ax=axis, legend=None)\n",
    "    \n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel(xlabel)\n",
    "    axis.set_ylabel(ylabel)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure to an image file.\n",
    "    if fileName is not None:\n",
    "        plt.savefig(\"images/\" + fileName)\n",
    "\n",
    "    # This should only be executed after plt.savefig(),\n",
    "    # otherwise the image saved will be blank.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_ngrams(ngrams_data, n=1, file_name=None):\n",
    "    \"\"\"\n",
    "    Plot the frequency of n-grams in a Series, for a given n.\n",
    "    \"\"\"\n",
    "    # Get all the tokens for the corpus in a list.\n",
    "    all_tokens_list = [token for tokens in ngrams_data for token in tokens]\n",
    "\n",
    "    ngrams = nltk.ngrams(all_tokens_list, n)\n",
    "\n",
    "    # Get the frequency of each term in the corpus and create a DataFrame from it.\n",
    "    ngram_frequency = nltk.FreqDist(ngrams)\n",
    "    ngrams_df = freq_dist_to_df(ngram_frequency, 5)\n",
    "    \n",
    "    ngram_name = \"Word\"  # default\n",
    "    if n == 2:\n",
    "        ngram_name = \"Bigram\"\n",
    "    elif n == 3:\n",
    "        ngram_name = \"Trigram\"\n",
    "\n",
    "    plot_frequency_df(ngrams_df, title=\"Top-10 Most-Common %ss for Entire Corpus\" % ngram_name,\n",
    "                      xlabel=\"Frequency\", ylabel=ngram_name, fileName=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=1, file_name=\"most-common-unigram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most-common words for the entire corpus don't really give us a huge amount of insight about the data. The most-common word is \"year\" but this could be used in any of the contexts of the document categories. Similarly for the word \"make\", \"people\" and \"time\". However, one word that does stand out is \"game\", which we would primarily associate with sport. However, it is possible that this word is applicable to other categories too, for example video games fall under the topic of technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngrams(df_handle.tokens, n=2, file_name=\"most-common-bigram.png\")\n",
    "plot_ngrams(df_handle.tokens, n=3, file_name=\"most-common-trigram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get more information from analysing the bigrams and trigrams than the single words (also known as *unigrams*). For example, we see that the some of the top bigrams and trigrams are \"bbc news\", \"bbc news website\" and \"bbc radio today\". This would seem to suggest that the source of our news articles is BBC News. In fact, this is also supported by the reference to former-British Prime Minister, Tony Blair, and former-British Conservative Party Leader, Michael Howard, who are referenced by top bigrams and trigrams too. \n",
    "\n",
    "Furthermore, we see that there are two bigrams that stand out for their association with particular document categories. \"Prime Minister\" is definitely associated with politics and \"Chief Executive\" is similarly associated with the business topic. We will investigate the importance of these n-grams later when dicussing the [most important terms and phrases](#most-important-n-grams) for classifying each topic. \n",
    "\n",
    "The fifth most common trigram is \"million dollar baby\". This could be a reference to the boxing film of the same name that won multiple Oscars in 2005. This trigram could definitely be associated with the entertainment category. However, seeing as it's a film that is related to sports, it could also be associated in some way to the sports category. We will investigate the [overlap between categories and their similarities](#category_similarity) later on in our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Frequency and Balance\n",
    "\n",
    "Now, let's look at the labels that we have been provided with for each of the documents in our corpus. There are five possible categories (or *classes*) that the label can represent: sport, entertainment, politics, technology and business. We also refer to our class labels as the *target label* of each document.\n",
    "\n",
    "We look at how many documents of each category appear in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df_handle.groupby(df_handle.category).category.count()\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise these absolute numbers to get a breakdown of the percentage frequency of each category. This makes it easy to compare the figures. We also plot a line, in orange, for the mean frequency, this is the frequency that we expect all categories to have if the data is perfectly uniformly distibuted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert category frequency to a percentage of all categories.\n",
    "class_distribution_normalised = (class_distribution / sum(class_distribution)) * 100\n",
    "\n",
    "# Plot a line representing the mean frequency of all classes.\n",
    "# This is the frequency each class would have if uniformly distributed.\n",
    "mean_freq = 100 / len(class_distribution)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.axhline(y=mean_freq, color='orange')\n",
    "class_distribution_normalised.plot.bar(ax=axis, rot=0)\n",
    "axis.set_xlabel(\"Target Class (Document Category)\")\n",
    "axis.set_ylabel(\"Frequency (% of Corpus)\")\n",
    "axis.set_title(\"Target Class Frequencies Across Corpus\")\n",
    "\n",
    "plt.savefig('images/target-class-frequencies.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a slighly unbalanced target class distribution. This means that not all target labels appear as often as each other. For example, the sport category is most common, whereas the entertainment category is the least common. When building our model, we need to take this into account, to make sure that we are not overly biasing our model based on the distribution of target classes in the training data. In extreme cases, our model could underfit the data and end up predicting the most common label (sport) each time. \n",
    "\n",
    "While this would be easy to spot and fix, it is possible that the class distribtion will affect the model in more subtle ways. One method we could use to avoid this would be to use the F-Measure as an evaluation metric for our models instead of just using Accuracy. We will investigate this later, in our section on model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Document Lengths\n",
    "\n",
    "Now, we look at some meta-statistics about each document. We compare the length of each document as well as the number of unique words in each document. We breakdown this analysis per-target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_handle['document_length'] = df_handle.tokens.apply(len)\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "df_handle.plot.hist(by='document_length', bins=5, ax=axis[0], figsize=(10,4), legend=None)\n",
    "axis[0].set_xlabel(\"Document Length (Number of Tokens)\")\n",
    "axis[0].set_title(\"Document Length Distribution\")\n",
    "\n",
    "clipped_content_length_data = df_handle[(df_handle.document_length > 100) & (df_handle.document_length < 500)]\n",
    "clipped_content_length_data.plot.hist(by='document_length', bins=5, ax=axis[1], figsize=(10, 4), legend=None)\n",
    "axis[1].set_xlabel(\"Document Length (Number of Tokens)\")\n",
    "axis[1].set_title(\"Document Length Distribution (clipped)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/document-length-distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first histogram on the left, we see that the overwhelming majority of documents have between 100 and 500 words. We don't get a huge amount of information from this plot so we replot the data, except that we filter the dataset so that we only include documents whose length is within this interval. This process is known as *clipping*. The clipped data is plotted on the right histogram.\n",
    "\n",
    "From the second histogram, we get more useful data. We see most of the documents have a length between 100 and 200 characters. It is important that we are aware that the distribution of the lengths of each document is not uniform. \n",
    "\n",
    "We also break this down for each document category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length_per_class = df_handle.groupby(df_handle.category).document_length.mean()\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "\n",
    "doc_length_per_class.plot.bar(ax=axis, rot=0)\n",
    "\n",
    "# We plot the average document length for reference.\n",
    "mean_doc_length = df_handle['document_length'].mean()\n",
    "axis.axhline(y=mean_doc_length, color='orange')\n",
    "\n",
    "axis.set_xlabel(\"Target Class (Document Category)\")\n",
    "axis.set_ylabel(\"Mean Document Length (num. tokens)\")\n",
    "axis.set_title(\"Mean Document Length per Target Class\")\n",
    "\n",
    "plt.savefig(\"images/document-length-per-class.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that we don't have uniform distribution of document length. This is important to recall, since, when we are using methods to vectorise the documents, if we don't take into account the documents' length, then we will be unfairly biasing longer or shorter documents (depending on the vectorising method). \n",
    "\n",
    "For example, if we are calculating the frequency of a term, comparing this across documents that have different lengths is unfair. A word that appears 10 times in a 50 word document is far more prominent that a word that appears 20 times in a 500 word document. So we need to make sure that we normalise our frequency calculation (i.e during TFIDF vectorisation) by the length of the doucment the term appears in. We will return to this point later when we are doing the document vectorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorisation\n",
    "\n",
    "TODO explain this process.\n",
    "Shouldn't we include the custom tokenizer here?\n",
    "\n",
    "Do some unit testing on toy examples to show that the vectorisation was successful (from feedback I got from Binh after HW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(norm='l2')\n",
    "document_token_strings = [' '.join(tokens) for tokens in df_handle.tokens]\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(document_token_strings).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = pd.DataFrame(\n",
    "    tfidf_vectors, columns=[tfidf_vectorizer.get_feature_names()], index=df_handle.index)\n",
    "tfidf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vector Analysis\n",
    "\n",
    "We now perform data analysis on the vectors that we have created for each of the documents. We want to explore trends or features of the documents and their categories to help improve our understanding of them. This will aid us in choosing and evaluating models in later sections.\n",
    "\n",
    "We are going to analyse the following:\n",
    "- Category Similarity\n",
    "- Most Important N-Grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Anchor for internal links -->\n",
    "<a id='category_similarity'></a>\n",
    "\n",
    "### Category Similarity \n",
    "\n",
    "Now we turn our attention to computing which categories are most- and least-similar to each other. \n",
    "We do this with the following steps:\n",
    "- Take each of our TFIDF document vectors\n",
    "- Group the documents by their category\n",
    "- Compute the mean TFIDF vector for each category\n",
    "- Compute the cosine distance between each mean vector\n",
    "- Calculate the closest vectors and the vectors furthest apart\n",
    "\n",
    "We use cosine distance as a measure of how similar each category is. Although distance measures how different each category is from each other, if we invert it then we get a similarity metric. This means that smaller distances imply greater similarity and larger distances mean less similarity. Cosine distance is a useful metric since it doesn't depend on the distance between the vectors in n-dimensional space, which could be unfairly biased by abnormally large vector-components. Instead, it depends on the angle between the vectors.\n",
    "\n",
    "We start by calculating the mean TFIDF document vectors from the TFIDF matrix and put them into their own dataframe so that we can analyse them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_handle.category.unique()\n",
    "mean_vector_df = pd.DataFrame(index=tfidf_data.columns)\n",
    "\n",
    "for category in categories:\n",
    "    mean_vector = tfidf_data[df_handle.category == category].mean(axis=0)\n",
    "    mean_vector_df[category] = mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the distance between the categories, and not between the n-grams in the corpus, we need to transpose our data frame. This swaps the rows and the columns in the data frame. This means that the rows will now correspond to the categories and the columns to the n-grams.\n",
    "\n",
    "We have done this because it is more efficient to add to a dataframe column-wise. Adding categories' data column-by-column and then transposing is more efficient than if we did the transposition first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose our data frame.\n",
    "mean_vector_transpose = mean_vector_df.T\n",
    "\n",
    "# Calculate cosine distance based on the tranposed data.\n",
    "pairwise_cosine_distance = sklearn.metrics.pairwise_distances(mean_vector_transpose, metric='cosine')\n",
    "\n",
    "pairwise_cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could read this matrix by eye, but this could get complicated. Instead, we define some helper functions to find the most-similar categories and the least-similar categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_closest_categories(array, n):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of the category-pairs that are most similar to each other.\n",
    "    The number of tuples returned is decided by the size parameter, n.\n",
    "    \"\"\"\n",
    "    return n_extreme_categories(array, n, multiplier=1)\n",
    "\n",
    "\n",
    "def n_furthest_categories(array, n):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of the category-pairs that are furthest away from each other.\n",
    "    The number of tuples returned is decided by the size parameter, n.\n",
    "    \"\"\"\n",
    "    return n_extreme_categories(array, n, multiplier=-1)\n",
    "\n",
    "\n",
    "def n_extreme_categories(array, n, multiplier):\n",
    "    \"\"\"\n",
    "    Return a list of tuples of the category pairs that have the most-extreme distances from \n",
    "    each other. Extreme can mean either smallest or largest distance from each other. \n",
    "    \n",
    "    In the case of the calculating the largest, we sort our array in reverse. This means that\n",
    "    we first have to multiply the array by -1, since numpy only provide an ascending-order \n",
    "    sorter.\n",
    "   \n",
    "    Note that we want to use built-in numpy library functions since they are most-efficient in \n",
    "    dealing with numpy arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We take the indices from 0 to 2n, with a step size of 2. This means that\n",
    "    # we skip every second index, which will be the same value as the index before it,\n",
    "    # given that the distance matrix is symmetric.\n",
    "    # For example, for the largest n=2, we take the first and third maximum elements, \n",
    "    # skipping the second since it will be the same as the first.\n",
    "    extreme_indices = np.argsort(multiplier * array.flatten())[0:2*n:2]\n",
    "\n",
    "    extreme_categories = []\n",
    "    \n",
    "    for ind in extreme_indices:\n",
    "        two_d_index = np.unravel_index(ind, array.shape)\n",
    "        \n",
    "        # Convert the tuple of indices to a tuple of the corresponding categories.\n",
    "        converted_tuple = tuple(map(lambda c: categories[c], two_d_index))\n",
    "        extreme_categories.append(converted_tuple)\n",
    "        \n",
    "    return extreme_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_furthest_categories(pairwise_cosine_distance, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the most-similar category-pairs we need to remove the zero values from our pairwise-distance array. The values are 0 when comparing a category to itself, but this doens't give us any valuable information. \n",
    "\n",
    "Instead, we will copy the pairwise-distance array and transform the zero-values to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_distances = pairwise_cosine_distance.copy()\n",
    "nonzero_distances[nonzero_distances == 0] = float(\"inf\")\n",
    "\n",
    "n_closest_categories(nonzero_distances, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the two most similar category-pairs are:\n",
    "- politics and buiness \n",
    "- business and tech\n",
    "\n",
    "And the two least similar category-pairs are:\n",
    "- sport and business\n",
    "- entertainment and business\n",
    "\n",
    "It is important the we recognise and compute these similarities when evaluating our model. It is a basic requirement that the model is able to distinguish between the least-similar categories of documents. In order to have an optimal model, it also needs to be able to tell apart the most-similar categories. \n",
    "\n",
    "However, when evaluating models, it is important to recognise that the category-pairs listed above are the most similar and, therefore, the hardest to tell apart. This means that we can still have a very good, high performance model, even if it is slightly confused between the most similar document categories.\n",
    "\n",
    "It is also interesting to note that the category of business is included in all of the most- and least-similar category-pairs. This indicates how complex and varied the defining-features of particular categories are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"most-important-words\"></a>\n",
    "### Most-Important Words\n",
    "Now we look at which words are the most-important in our dataset. We define importance by the TFIDF metric. These will be the words that are most indicative of a document's category.\n",
    "\n",
    "#### Across Entire Corpus\n",
    "\n",
    "First of all, though, let's look at the most important words throughout the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean TFIDF vector for the entire corpus.\n",
    "tfidf_mean = tfidf_data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 10 largest TFIDF-valued terms\n",
    "most_important_words = tfidf_mean.nlargest(10)\n",
    "\n",
    "most_important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within Each Category\n",
    "\n",
    "Now, we look at the most important words within each category, instead of across the entire corpus.\n",
    "\n",
    "Note that we have already calculated the mean TFIDF document vectors for each category in the [Category Similarity](#category_similarity) section above. We are going to reuse them since there is no need to duplicate the data, which will increase the computational time of our notebook unnecesarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(len(categories), sharex=True)\n",
    "\n",
    "# Keep a list of all terms that appear in the top ten for each category.\n",
    "most_important_terms = []\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    category_vector = mean_vector_transpose.iloc[i]\n",
    "    \n",
    "    category_vector.nlargest(10).plot.barh(ax=axis[i], figsize=(10, 15))\n",
    "    \n",
    "    most_important_terms.extend(category_vector.nlargest(10).index)\n",
    "    \n",
    "    axis_title = \"Most Important N-Grams for Category %s\"\n",
    "    axis[i].set_title(axis_title % category_vector.name.title())\n",
    "    \n",
    "axis[len(categories) - 1].set_xlabel(\"Importance (TFIDF Value)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/most-important-terms-per-category.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bar charts give us a wealth of insight into the defining words for each category. For example, by far the most important term is \"film\", which is indicative of the entertainment category. This means that a good model will identify the occurences of the word \"film\" in a document and make it more likely to classify that document as being about entertainment. \n",
    "\n",
    "We also get lots of information about the key words in each category. To start with, they are all unigrams (the lack of bigram or trigram phrases is discussed above). It is also interesting to note that the categories appear to be fairly disjoint from each other, based on their top ten most important words. We can see this by the fact that the only terms that is in the top ten in more than one category are:\n",
    "- \"game\", appearing for both the sport and tech categories\n",
    "- \"year\", appearing for both the business and entertainment categories\n",
    "\n",
    "We show this, computationally, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter(most_important_terms)\n",
    "\n",
    "for word in counter:\n",
    "    if counter[word] > 1:\n",
    "        print(\"%s: %s\" % (word, counter[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when considering all terms, not just the top ten, we have seen that the categories are more aligned. This is discussed in the [Category Similarity section](#category_similarity)  above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initial_models'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides many different models for classification. Rather than try to guess which ones would be best for our problem, we will test and analyse a large selection of them. We will then pick the three best models to do further analysis on. \n",
    "\n",
    "In order to thoroughly test our models, we will split our data into two sets:\n",
    "- a *training set* that the models are built using; and\n",
    "- a *test set* that allows us to assesss the performance of a model on unseen data\n",
    "\n",
    "We will test each of the models by using cross-validation and using accuracy as a scoring metric. We don't use the F-measure to assess our models yet, since there is a quirk in Sklearn. The F-measure is set to 0 when a model is making predictions on a test set that doesn't contain all of the categories at least once.\n",
    "\n",
    "However, we do want to use the F-measure later on in our evaluations, so we ensure that the test split of our data contains at least every category at least once.\n",
    "\n",
    "We perform this train-test split now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(df_handle.category.unique())\n",
    "\n",
    "keep_going = True\n",
    "while keep_going:\n",
    "    # Put \\ at the end of the line so the statement is continued over the next line.\n",
    "    train_input_vectors, test_input_vectors, train_labels, test_labels = \\\n",
    "        train_test_split(tfidf_vectors, df_handle['category'])\n",
    "        \n",
    "    # If we've found a test split that contains all of the labels \n",
    "    # (thus allowing us to do F-measure evaluations), then we stop.\n",
    "    if len(test_labels.unique()) == num_categories:\n",
    "        keep_going = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all of the models that we are going to train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training and cross-validation process takes a long time. Results from a previous execution are included in a markdown cell below. By default, the process is set not to run again. If you would like to run the process, set the constant below to true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ALL_MODELS_EVAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you do re-run this process, your result may differ slightly from those in the markdown cell due to the nondeterministic splitting of the dataset during the cross validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Naive Bayes\",\n",
    "    \"Logistic Regression\",\n",
    "    \"AdaBoost\",\n",
    "    \"Random Forest\",  \n",
    "    \"Linear SVM\", \n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(6),\n",
    "    MultinomialNB(alpha=1.0),\n",
    "    LogisticRegression(multi_class='multinomial', solver='newton-cg'),\n",
    "    AdaBoostClassifier(),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1), \n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "]\n",
    "\n",
    "if RUN_ALL_MODELS_EVAL:\n",
    "    for name, classifier in zip(names, classifiers):    \n",
    "        k_folds = 10\n",
    "        scoring = 'accuracy'\n",
    "        scores = cross_val_score(classifier, train_input_vectors, train_labels, \n",
    "                                 cv=k_folds, scoring=scoring)\n",
    "\n",
    "        average_accuracy = np.mean(scores)\n",
    "\n",
    "        model_accuracy[name] = average_accuracy\n",
    "\n",
    "        print(\"%s ave. %s = %.4f\" % (name, scoring, average_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of a previous execution of this process were:\n",
    "\n",
    "| Model | Accuracy |\n",
    "| --- | --- |\n",
    "| Logistic Regression | 0.9735 |\n",
    "| Naive Bayes | 0.9605 |\n",
    "| Nearest Neighbors | 0.9347 |\n",
    "| RBF SVM | 0.9149 |\n",
    "| AdaBoost ave. |0.6907 |\n",
    "| Decision Tree | 0.6236 |\n",
    "| Random Forest | 0.3128 |\n",
    "| Linear SVM | 0.2374 |\n",
    "\n",
    "We see that the highest-performing models are the Logistic Regression, Naive Bayes and Nearest Neighbour classifiers. We will now do further investigation and analysis on these models in order to find the optimal model for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_discuss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe model, process\n",
    "- Include any relevant formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_optimize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, have the code for this but comment it out. Include the results in a markdown cell.\n",
    "\n",
    "train_input vectors and train_labels.\n",
    "\n",
    "Measure performance based on scoring='accuracy' metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_k = 0\n",
    "best_k_accuracy = 0\n",
    "\n",
    "for k in range(1, 10):\n",
    "    k_folds = 10\n",
    "    scores = cross_val_score(KNeighborsClassifier(k), train_input_vectors, train_labels, \n",
    "                             cv=k_folds, scoring='f1_weighted')\n",
    "    \n",
    "    average_accuracy = np.mean(scores)\n",
    "    \n",
    "    if average_accuracy > best_k_accuracy:\n",
    "        best_k_accuracy = average_accuracy\n",
    "        best_k = k\n",
    "\n",
    "    print(\"KNN (k=%d) average accuracy (%d-fold x-val): %f\" \n",
    "          % (k, k_folds, average_accuracy))\n",
    "    \n",
    "\n",
    "print(\"Best k value is %d\" % best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "KNN (k=1) average accuracy (10-fold x-val): 0.893683\n",
    "\n",
    "KNN (k=2) average accuracy (10-fold x-val): 0.894864\n",
    "\n",
    "KNN (k=3) average accuracy (10-fold x-val): 0.905719\n",
    "\n",
    "KNN (k=4) average accuracy (10-fold x-val): 0.917850\n",
    "\n",
    "KNN (k=5) average accuracy (10-fold x-val): 0.927338\n",
    "\n",
    "KNN (k=6) average accuracy (10-fold x-val): 0.938170\n",
    "\n",
    "KNN (k=7) average accuracy (10-fold x-val): 0.938157\n",
    "\n",
    "KNN (k=8) average accuracy (10-fold x-val): 0.938826\n",
    "\n",
    "KNN (k=9) average accuracy (10-fold x-val): 0.936350\n",
    "\n",
    "KNN (k=10) average accuracy (10-fold x-val): 0.936343\n",
    "\n",
    "Best k value is 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model Learning\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 8, metric = 'cosine')\n",
    "knn_model.fit(train_input_vectors,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply KNN Model to test data\n",
    "knn_predicted_labels = knn_model.predict(test_input_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full train_input_vectors set using the train_labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m1_eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test_input_vectors set and compare to the test_labels.\n",
    "\n",
    "- Confusion Matrix\n",
    "- Accuracy, Precision, Recall, F1-Score on the Test Split.\n",
    "     - Graph these for each of the categories and make sure that its roughly even across all of the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_accuracy = accuracy_score(test_labels, knn_predicted_labels)\n",
    "print('Accuracy rate for KNN model: {:0.2f}%'.format(knn_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Rate\n",
    "knn_error   = 1 - knn_accuracy\n",
    "print('Error rate for KNN model: {:0.2f}%'.format(knn_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the confusion matrix to a dataframe\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_to_df(cm, labels):\n",
    "    df = pd.DataFrame()\n",
    "    # rows\n",
    "    for i, row_label in enumerate(labels):\n",
    "        rowdata={}\n",
    "        # columns\n",
    "        for j, col_label in enumerate(labels): \n",
    "            rowdata[col_label] = cm[i,j]\n",
    "        df = df.append(pd.DataFrame.from_dict({row_label:rowdata}, orient='index'))\n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check here for incorrect y-test parameter ... The matrix isn't quite right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_conf_matrix = confusion_matrix(test_labels, knn_predicted_labels)\n",
    "knn_conf_matrix_df = cm_to_df(knn_conf_matrix,['neg','pos'])\n",
    "(knn_conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    width, height = cm.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix for KNN Classifier')\n",
    "plot_confusion_matrix(knn_conf_matrix, ['NEG', 'POS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Statistic Report for KNN Classifier')\n",
    "print(classification_report(test_labels, knn_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2: Naive Bayes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_discuss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Naive Bayes Classifier* is a eager learning algorithm which uses probababilities to predict the most likely class for a given example.\n",
    "\n",
    "To calculate the probability that a document, with features (i.e. TDIDF values) $\\{f_1, \\ldots , f_n\\}$ is in category $c$, we use **Bayes Theorem**. This is given by:\n",
    "$$ \\text{Pr}(c \\;|\\; \\{f_1, \\ldots , f_n\\}) = \\frac{\\text{Pr}(\\{f_1, \\ldots , f_n\\} \\;|\\; c) \\ \\text{Pr}(c)}{\\text{Pr}(\\{f_1, \\ldots , f_n\\})} $$\n",
    "\n",
    "Since it can be very difficult to calculate $\\text{Pr}(\\{f_1, \\ldots , f_n\\} \\;|\\; c)$, we make an assumption that all the features are **conditionally independent** of each other. This is a rather naive assumption (hence the name Naive Bayes), however it can be rather effective.\n",
    "\n",
    "This assumption gives us an equation that is much easier to calculate, since now\n",
    "$$\\text{Pr}(\\{f_1, \\ldots , f_n\\} \\;|\\; c) = \\prod_i \\text{Pr}(\\ f_i \\;|\\; c)$$\n",
    "\n",
    "The training stage of the algorithm consists of calculating the components of these equations. When we need to classify an example document, we use its feature values to calculate the probability of the document having each individual category. Then we return the category with the highest probability as our prediction.\n",
    "\n",
    "There are a number of advantages to using a Naive Bayes classifier, including:\n",
    "- Good time and memory performance: it's gast and needs little storage\n",
    "- It can dilute irrelevant features\n",
    "- It is the optimal solution if the naive assumption is true\n",
    "\n",
    "However, in some domains, the naive assumption is not true and this can lead to poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('The search range is: ' + str(np.linspace(0.0001,0.0002,11)))\n",
    "bayes_clf_val = MultinomialNB()\n",
    "bayes_parameters = {'alpha': (np.linspace(0.0001,0.0002,11))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_gs_clf = GridSearchCV(bayes_clf_val, param_grid = bayes_parameters, cv = 10, scoring = 'accuracy')\n",
    "bayes_gs_clf = bayes_gs_clf.fit(train_input_vectors, train_labels)\n",
    "NB_parameters = bayes_gs_clf.best_params_\n",
    "print('The best parameter is: ', NB_parameters['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Model Learning\n",
    "bayes_model = MultinomialNB(alpha = 0.0001)\n",
    "bayes_model.fit(train_input_vectors,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Bayes Model to test data\n",
    "bayes_predicted_labels = bayes_model.predict(test_input_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m2_eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "bayes_accuracy = accuracy_score(test_labels, bayes_predicted_labels)\n",
    "print('Accuracy rate for Naive Bayes model: {:0.2f}%'.format(bayes_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Rate\n",
    "bayes_error   = 1 - bayes_accuracy\n",
    "print('Error rate for Naive Bayes model: {:0.2f}%'.format(bayes_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check here for incorrect y-test parameter ... The matrix isn't quite right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_conf_matrix = confusion_matrix(test_labels, bayes_predicted_labels)\n",
    "bayes_conf_matrix_df = cm_to_df(bayes_conf_matrix,['neg','pos'])\n",
    "(bayes_conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix for Naive Bayes Classifier')\n",
    "plot_confusion_matrix(bayes_conf_matrix, ['NEG', 'POS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statistic Report for Naive Bayes Classifier')\n",
    "print(classification_report(test_labels, bayes_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_discuss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_optimize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('The search range is: ' + str(np.linspace(50,75,26)))\n",
    "log_reg_clf_val = LogisticRegression()\n",
    "log_reg_parameters = {'C': (np.linspace(50,75,26))}, #\"penalty\":[\"l1\",\"l2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_gs_clf = GridSearchCV(log_reg_clf_val, param_grid = log_reg_parameters, cv = 10, scoring = 'accuracy')\n",
    "log_reg_gs_clf = log_reg_gs_clf.fit(train_input_vectors, train_labels)\n",
    "LR_parameters = log_reg_gs_clf.best_params_\n",
    "print('The best parameter is: ', LR_parameters['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_gs_clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model Learning\n",
    "log_reg_model = LogisticRegression(C = 50)\n",
    "log_reg_model.fit(train_input_vectors,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Logistic Regression Model to test data\n",
    "log_reg_predicted_labels = log_reg_model.predict(test_input_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m3_eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "log_reg_accuracy = accuracy_score(test_labels, log_reg_predicted_labels)\n",
    "print('Accuracy rate for Logistic Regression model: {:0.2f}%'.format(log_reg_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Rate\n",
    "log_reg_error   = 1 - log_reg_accuracy\n",
    "print('Error rate for Logistic Regression model: {:0.2f}%'.format(log_reg_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check here for incorrect y-test parameter ... The matrix isn't quite right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_conf_matrix = confusion_matrix(test_labels, log_reg_predicted_labels)\n",
    "log_reg_conf_matrix_df = cm_to_df(log_reg_conf_matrix,['neg','pos'])\n",
    "(log_reg_conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix for Logistic Regression Classifier')\n",
    "plot_confusion_matrix(log_reg_conf_matrix, ['NEG', 'POS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statistic Report for Logistic Regression Classifier')\n",
    "print(classification_report(test_labels, log_reg_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='apply_final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Final Model to the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain our final model on entirety of training set. Include the train split as well as the test split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
